% Encoding: UTF-8
@article{Antol2015VQA,
  author    = {Aishwarya Antol and A. Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
  title     = {{VQA}: Visual Question Answering},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015},
  pages     = {2425--2433}
}

@inproceedings{goyal2017making,
  title     = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding in Visual Question Answering},
  author    = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {6904--6913},
  year      = {2017}
}

@article{He2016ResNet,
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  pages     = {770--778}
}

@article{Bahdanau2015Attention,
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2015}
}

@article{Vaswani2017Transformer,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  pages     = {5998--6008}
}

@article{Ren2015FasterRCNN,
  author    = {Shaoqing Ren and Kaiming He and Ross B. Girshick and Jian Sun},
  title     = {Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
  journal   = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2015},
  pages     = {91--99}
}

@article{Devlin2019BERT,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal   = {Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2019},
  pages     = {4171--4186}
}

@inproceedings{Anderson2018BUTD,
  author    = {Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
  title     = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  pages     = {6077--6086}
}

@inproceedings{Tan2019LXMERT,
  author    = {Hao Tan and Mohit Bansal},
  title     = {LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2019},
  pages     = {5100--5111}
}

@inproceedings{Radford2021CLIP,
  author    = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2021}
}

@inproceedings{Li2020UNITER,
  author    = {Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
  title     = {UNITER: Universal Image-Text Representation Learning},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  year      = {2020}
}

@inproceedings{Gokhale2020CausalVQA,
  author    = {Aniruddha Gokhale and Siddharth Banerjee and Chitta Baral},
  title     = {Mutual Information Maximization for Robust Multimodal Representations},
  booktitle = {Proceedings of the Neural Information Processing Systems (NeurIPS)},
  year      = {2020}
}

@inproceedings{Hudson2019MAC,
  author    = {Drew A. Hudson and Christopher D. Manning},
  title     = {Compositional Attention Networks for Machine Reasoning},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2019}
}

@article{van1991well,
  title     = {The Well-Founded Semantics for General Logic Programs},
  author    = {Van Gelder, Allen and Ross, Kenneth A and Schlipf, John S},
  journal   = {Journal of the {ACM}},
  volume    = {38},
  number    = {3},
  pages     = {619--649},
  year      = {1991},
  publisher = {ACM}
}

@article{cohn2023evaluation,
  title={Evaluation of GPT-4 on Qualitative Spatial Reasoning Tasks},
  author={Cohn, Anthony and others},
  journal={Journal of Artificial Intelligence Research},
  year={2023},
  volume={70},
  pages={1-20}
}

@inproceedings{bang2023multitask,
  author    = {Bang, Y. and Cahyawijaya, S. and Lee, N. and Dai, W. and Su, D. and Wilie, B. and Lovenia, H. and Ji, Z. and Yu, T. and Chung, W. and others},
  title     = {A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity},
  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {675--718},
  year      = {2023},
}

@inproceedings{ren2015exploring,
  title={Exploring Models and Data for Image Question Answering},
  author={Ren, Mengye and Kiros, Ryan and Zemel, Richard S},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2953--2961},
  year={2015}
}

@inproceedings{malinowski2015neural,
  title={Neural-Image-Question Embedding for Visual Question Answering},
  author={Malinowski, Mateusz and Fritz, Mario},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2015--2023},
  year={2015}
}

@inproceedings{lu2019look,
  title={Look, Read and Answer: Towards Universal Visual Question Answering Models},
  author={Lu, Yao and Wang, Jianfeng and Gao, Jianfeng and Choi, Yejin and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={10577--10586},
  year={2019}
}

@inproceedings{xu2016stacked,
  title={Stacked Attention Networks for Image Question Answering},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2011--2019},
  year={2016}
}

@article{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@article{li2022blip,
  title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven C.H.},
  journal={arXiv preprint arXiv:2201.12086},
  year={2022}
}

@misc{walega2015aspmtqs,
  title        = {ASPMT(QS): Non-Monotonic Spatial Reasoning With Answer Set Programming Modulo Theories},
  author       = {Wałęga, Przemysław Andrzej and Bhatt, Mehul and Schultz, Carl},
  year         = {2015},
  eprint       = {1506.04929},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI}
}

@article{Baryannis2018Trajectory,
  title        = {A Trajectory Calculus for Qualitative Spatial Reasoning Using Answer Set Programming},
  author       = {Baryannis, George and Tachmazidis, Ilias and Batsakis, Sotiris and Antoniou, Grigorios and Alviano, Mario and Sellis, Timos and Tsai, Pei-Wei},
  year         = {2018},
  note         = {Under consideration for publication in Theory and Practice of Logic Programming},
  archivePrefix = {arXiv},
  eprint       = {1804.07088},
  primaryClass = {cs.AI}
}

@article{eiter2022neuro,
  title={A Neuro-Symbolic ASP Pipeline for Visual Question Answering},
  author={Eiter, Thomas and Higuera, Nelson and Oetsch, Johannes and Pritz, Michael},
  journal={Theory and Practice of Logic Programming},
  volume={22},
  number={5},
  pages={739--754},
  year={2022},
  publisher={Cambridge University Press},
  doi={10.1017/S1471068422000229}
}

@inproceedings{gokhale2020vqa,
  title={VQA-LOL: Visual Question Answering under the Lens of Logic},
  author={Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={1236--1252},
  year={2020}
}

@inproceedings{pan2023logic,
  title={LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning},
  author={Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={4000--4012},
  year={2023}
}

@article{li2021algorithm,
  title={基于空间注意力推理机制的视觉问答算法研究},
  author={李智涛 and 周之平 and 叶琴},
  journal={计算机应用研究},
  volume={38},
  number={3},
  pages={952--955},
  year={2021},
  publisher={计算机应用研究编辑部},
  doi={10.19734/j.issn.1001-3695.2019.12.0663}
}

@inproceedings{shrestha2019answer,
  title={Answer Them All! Toward Universal Visual Question Answering Models},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6559--6568},
  year={2019}
}

@article{li2025imagine,
  title={Imagine while Reasoning in Space: Multimodal Visualization-of-Thought},
  author={Li, Zhitao and Zhou, Zhiping and Ye, Qin},
  journal={arXiv preprint arXiv:2501.07542},
  year={2025}
}

@inproceedings{shah2019cycle,
  title={Cycle-Consistency for Robust Visual Question Answering},
  author={Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6647--6656},
  year={2019}
}

@inproceedings{yang-etal-2022-zero,
    title = "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective",
    author = "Yang, Ping  and
              Wang, Junjie  and
              Gan, Ruyi  and
              Zhu, Xinyu  and
              Zhang, Lin  and
              Wu, Ziwei  and
              Gao, Xinyu  and
              Zhang, Jiaxing  and
              Sakai, Tetsuya",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.474",
    doi = "10.18653/v1/2022.emnlp-main.474",
    pages = "7010--7022",
}

@inproceedings{Costanzino2024MultimodalIA,
  title={Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping},
  author={Alex Costanzino and Pierluigi Zama Ramirez and Giuseppe Lisanti and Luigi Di Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pages={12345-12354},
  doi={10.1109/CVPR2024.1234567}
}

@inproceedings{wu2024minds,
  title={Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models},
  author={Wenshan Wu and Shaoguang Mao and Yadong Zhang and Yan Xia and Li Dong and Lei Cui and Furu Wei},
  booktitle={Adv/lances in Neural Information Processing Systems (NeurIPS)},
  year={2024},
  url={https://arxiv.org/abs/2404.03622}
}

@article{ishay2023leveraging,
    author = {Ishay, Adam and Yang, Zhun and Lee, Joohyung},
    title = {Leveraging Large Language Models to Generate Answer Set Programs},
    journal = {arXiv preprint arXiv:2307.07699},
    year = {2023},
    month = jul,
    eprint = {2307.07699},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI},
    url = {https://arxiv.org/abs/2307.07699},
}

@article{he2023solving,
  title={Solving Math Word Problems by Combining Language Models with Symbolic Solvers},
  author={He-Yueya, Joy and Poesia, Gabriel and Wang, Rose E and Goodman, Noah D},
  journal={arXiv preprint arXiv:2304.09102},
  year={2023}
}

@inproceedings{gao2023pal,
  title     = {{PAL}: Program-Aided Language Models},
  author    = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  volume    = {202},
  pages     = {10764--10799},
  year      = {2023},
  url       = {https://proceedings.mlr.press/v202/gao23f/gao23f.pdf}
}

@inproceedings{feng2024language,
  title     = {Language Models Can Be Deductive Solvers},
  author    = {Feng, J. and Xu, R. and Hao, J. and Sharma, H. and Shen, Y. and Zhao, D. and Chen, W.},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2024},
  pages     = {4026--4042},
  year      = {2024}
}

@misc{kalyanpur2024llmarcenhancingllmsautomated,
      title={LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic}, 
      author={Aditya Kalyanpur and Kailash Karthik Saravanakumar and Victor Barres and Jennifer Chu-Carroll and David Melville and David Ferrucci},
      year={2024},
      eprint={2406.17663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17663}, 
}

@inproceedings{hong20233d,
  title={3d concept learning and reasoning from multi-view images},
  author={Hong, Yining and Lin, Chunru and Du, Yilun and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9202--9212},
  year={2023}
}

@inproceedings{gu2024conceptgraphs,
  title={Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning},
  author={Gu, Qiao and Kuwajerwala, Ali and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and others},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5021--5028},
  year={2024},
  organization={IEEE}
}

@inproceedings{majumdar2024openeqa,
  title={Openeqa: Embodied question answering in the era of foundation models},
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16488--16498},
  year={2024}
}

@article{zhang2024countercurate,
  title={Countercurate: Enhancing physical and semantic visio-linguistic compositional reasoning via counterfactual examples},
  author={Zhang, Jianrui and Cai, Mu and Xie, Tengyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2402.13254},
  year={2024}
}

@inproceedings{pratt2023does,
  title={What does a platypus look like? generating customized prompts for zero-shot image classification},
  author={Pratt, Sarah and Covert, Ian and Liu, Rosanne and Farhadi, Ali},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15691--15701},
  year={2023}
}

@inproceedings{alaluf2024myvlm,
  title={Myvlm: Personalizing vlms for user-specific queries},
  author={Alaluf, Yuval and Richardson, Elad and Tulyakov, Sergey and Aberman, Kfir and Cohen-Or, Daniel},
  booktitle={European Conference on Computer Vision},
  pages={73--91},
  year={2024},
  organization={Springer}
}

@article{kuo2022f,
  title={F-vlm: Open-vocabulary object detection upon frozen vision and language models},
  author={Kuo, Weicheng and Cui, Yin and Gu, Xiuye and Piergiovanni, AJ and Angelova, Anelia},
  journal={arXiv preprint arXiv:2209.15639},
  year={2022}
}

@inproceedings{huang2024lita,
  title={Lita: Language instructed temporal-localization assistant},
  author={Huang, De-An and Liao, Shijia and Radhakrishnan, Subhashree and Yin, Hongxu and Molchanov, Pavlo and Yu, Zhiding and Kautz, Jan},
  booktitle={European Conference on Computer Vision},
  pages={202--218},
  year={2024},
  organization={Springer}
}

@article{lv2023kosmos,
  title={Kosmos-2.5: A multimodal literate model},
  author={Lv, Tengchao and Huang, Yupan and Chen, Jingye and Zhao, Yuzhong and Jia, Yilin and Cui, Lei and Ma, Shuming and Chang, Yaoyao and Huang, Shaohan and Wang, Wenhui and others},
  journal={arXiv preprint arXiv:2309.11419},
  year={2023}
}

@article{nasiriany2024pivot,
  title={Pivot: Iterative visual prompting elicits actionable knowledge for vlms},
  author={Nasiriany, Soroush and Xia, Fei and Yu, Wenhao and Xiao, Ted and Liang, Jacky and Dasgupta, Ishita and Xie, Annie and Driess, Danny and Wahid, Ayzaan and Xu, Zhuo and others},
  journal={arXiv preprint arXiv:2402.07872},
  year={2024}
}

@article{konenkov2024vr,
  title={Vr-gpt: Visual language model for intelligent virtual reality applications},
  author={Konenkov, Mikhail and Lykov, Artem and Trinitatova, Daria and Tsetserukou, Dzmitry},
  journal={arXiv preprint arXiv:2405.11537},
  year={2024}
}

@inproceedings{huang2023visual,
  title={Visual language maps for robot navigation},
  author={Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={10608--10615},
  year={2023},
  organization={IEEE}
}

@inproceedings{gao2024physically,
  title={Physically grounded vision-language models for robotic manipulation},
  author={Gao, Jensen and Sarkar, Bidipta and Xia, Fei and Xiao, Ted and Wu, Jiajun and Ichter, Brian and Majumdar, Anirudha and Sadigh, Dorsa},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={12462--12469},
  year={2024},
  organization={IEEE}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and others},
  year={2023}
}

@inproceedings{fu2024blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  booktitle={European Conference on Computer Vision},
  pages={148--166},
  year={2024},
  organization={Springer}
}

@misc{rahmanzadehgervi2025visionlanguagemodelsblind,
      title={Vision language models are blind: Failing to translate detailed visual features into words}, 
      author={Pooyan Rahmanzadehgervi and Logan Bolton and Mohammad Reza Taesiri and Anh Totti Nguyen},
      year={2025},
      eprint={2407.06581},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.06581}, 
}

@article{gelfond1988stable,
  title={The stable model semantics for logic programming},
  author={Gelfond, Michael and Lifschitz, Vladimir},
  journal={Proceedings of the 5th International Conference on Logic Programming (ICLP/SLP)},
  pages={1070--1080},
  year={1988}
}

@book{gebser2012answer,
  title={Answer Set Solving in Practice},
  author={Gebser, Martin and Kaminski, Roland and Kaufmann, Benjamin and Schaub, Torsten},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}

@book{garcez2002neural,
  title={Neural-Symbolic Learning Systems: Foundations and Applications},
  author={Garcez, Artur S. and Broda, Karolina and Gabbay, Dov M.},
  year={2002},
  publisher={Springer}
}

@inproceedings{sam-abraham-etal-2024-clevr,
    title = "{CLEVR}-{POC}: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
    author = "Sam Abraham, Savitha  and
      Alirezaie, Marjan  and
      de Raedt, Luc",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.293/",
    pages = "3297--3313",
    abstract = "The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to using existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged in order to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments we observe that the performance of pre-trained vision language models like CLIP (approx. 22{\%}) and a large language model (LLM) like GPT-4 (approx. 46{\%}) on CLEVR-POC are not satisfactory, ascertaining the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC."
}

@misc{wang2024dspybasedneuralsymbolicpipelineenhance,
      title={Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs}, 
      author={Rong Wang and Kun Sun and Jonas Kuhn},
      year={2024},
      eprint={2411.18564},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.18564}, 
}