% Encoding: UTF-8
@article{Antol2015VQA,
  author    = {Aishwarya Antol and A. Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
  title     = {{VQA}: Visual Question Answering},
  journal   = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015},
  pages     = {2425--2433}
}

@inproceedings{goyal2017making,
  title     = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding in Visual Question Answering},
  author    = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {6904--6913},
  year      = {2017}
}

@article{He2016ResNet,
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  pages     = {770--778}
}

@article{Bahdanau2015Attention,
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2015}
}

@article{Vaswani2017Transformer,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  pages     = {5998--6008}
}

@article{Ren2015FasterRCNN,
  author    = {Shaoqing Ren and Kaiming He and Ross B. Girshick and Jian Sun},
  title     = {Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
  journal   = {Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2015},
  pages     = {91--99}
}

@article{Devlin2019BERT,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal   = {Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2019},
  pages     = {4171--4186}
}

@inproceedings{Anderson2018BUTD,
  author    = {Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
  title     = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  pages     = {6077--6086}
}

@inproceedings{Tan2019LXMERT,
  author    = {Hao Tan and Mohit Bansal},
  title     = {LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2019},
  pages     = {5100--5111}
}

@inproceedings{Radford2021CLIP,
  author    = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2021}
}

@inproceedings{Li2020UNITER,
  author    = {Liunian Harold Li and Mark Yatskar and Da Yin and Cho-Jui Hsieh and Kai-Wei Chang},
  title     = {UNITER: Universal Image-Text Representation Learning},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  year      = {2020}
}

@inproceedings{Gokhale2020CausalVQA,
  author    = {Aniruddha Gokhale and Siddharth Banerjee and Chitta Baral},
  title     = {Mutual Information Maximization for Robust Multimodal Representations},
  booktitle = {Proceedings of the Neural Information Processing Systems (NeurIPS)},
  year      = {2020}
}

@inproceedings{Hudson2019MAC,
  author    = {Drew A. Hudson and Christopher D. Manning},
  title     = {Compositional Attention Networks for Machine Reasoning},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2019}
}

@article{van1991well,
  title     = {The Well-Founded Semantics for General Logic Programs},
  author    = {Van Gelder, Allen and Ross, Kenneth A and Schlipf, John S},
  journal   = {Journal of the {ACM}},
  volume    = {38},
  number    = {3},
  pages     = {619--649},
  year      = {1991},
  publisher = {ACM}
}

@article{cohn2023evaluation,
  title={Evaluation of GPT-4 on Qualitative Spatial Reasoning Tasks},
  author={Cohn, Anthony and others},
  journal={Journal of Artificial Intelligence Research},
  year={2023},
  volume={70},
  pages={1-20}
}

@inproceedings{bang2023multitask,
  author    = {Bang, Y. and Cahyawijaya, S. and Lee, N. and Dai, W. and Su, D. and Wilie, B. and Lovenia, H. and Ji, Z. and Yu, T. and Chung, W. and others},
  title     = {A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity},
  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {675--718},
  year      = {2023},
}

@inproceedings{ren2015exploring,
  title={Exploring Models and Data for Image Question Answering},
  author={Ren, Mengye and Kiros, Ryan and Zemel, Richard S},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2953--2961},
  year={2015}
}

@inproceedings{malinowski2015neural,
  title={Neural-Image-Question Embedding for Visual Question Answering},
  author={Malinowski, Mateusz and Fritz, Mario},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2015--2023},
  year={2015}
}

@inproceedings{lu2019look,
  title={Look, Read and Answer: Towards Universal Visual Question Answering Models},
  author={Lu, Yao and Wang, Jianfeng and Gao, Jianfeng and Choi, Yejin and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={10577--10586},
  year={2019}
}

@inproceedings{xu2016stacked,
  title={Stacked Attention Networks for Image Question Answering},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2011--2019},
  year={2016}
}

@article{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}

@article{li2022blip,
  title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven C.H.},
  journal={arXiv preprint arXiv:2201.12086},
  year={2022}
}

@misc{walega2015aspmtqs,
  title        = {ASPMT(QS): Non-Monotonic Spatial Reasoning With Answer Set Programming Modulo Theories},
  author       = {Wałęga, Przemysław Andrzej and Bhatt, Mehul and Schultz, Carl},
  year         = {2015},
  eprint       = {1506.04929},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI}
}

@article{Baryannis2018Trajectory,
  title        = {A Trajectory Calculus for Qualitative Spatial Reasoning Using Answer Set Programming},
  author       = {Baryannis, George and Tachmazidis, Ilias and Batsakis, Sotiris and Antoniou, Grigorios and Alviano, Mario and Sellis, Timos and Tsai, Pei-Wei},
  year         = {2018},
  note         = {Under consideration for publication in Theory and Practice of Logic Programming},
  archivePrefix = {arXiv},
  eprint       = {1804.07088},
  primaryClass = {cs.AI}
}

@article{eiter2022neuro,
  title={A Neuro-Symbolic ASP Pipeline for Visual Question Answering},
  author={Eiter, Thomas and Higuera, Nelson and Oetsch, Johannes and Pritz, Michael},
  journal={Theory and Practice of Logic Programming},
  volume={22},
  number={5},
  pages={739--754},
  year={2022},
  publisher={Cambridge University Press},
  doi={10.1017/S1471068422000229}
}

@inproceedings{gokhale2020vqa,
  title={VQA-LOL: Visual Question Answering under the Lens of Logic},
  author={Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={1236--1252},
  year={2020}
}

@inproceedings{pan2023logic,
  title={LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning},
  author={Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={4000--4012},
  year={2023}
}

@article{li2021algorithm,
  title={基于空间注意力推理机制的视觉问答算法研究},
  author={李智涛 and 周之平 and 叶琴},
  journal={计算机应用研究},
  volume={38},
  number={3},
  pages={952--955},
  year={2021},
  publisher={计算机应用研究编辑部},
  doi={10.19734/j.issn.1001-3695.2019.12.0663}
}

@inproceedings{shrestha2019answer,
  title={Answer Them All! Toward Universal Visual Question Answering Models},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6559--6568},
  year={2019}
}

@article{li2025imagine,
  title={Imagine while Reasoning in Space: Multimodal Visualization-of-Thought},
  author={Li, Zhitao and Zhou, Zhiping and Ye, Qin},
  journal={arXiv preprint arXiv:2501.07542},
  year={2025}
}

@inproceedings{shah2019cycle,
  title={Cycle-Consistency for Robust Visual Question Answering},
  author={Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6647--6656},
  year={2019}
}

@inproceedings{yang-etal-2022-zero,
    title = "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective",
    author = "Yang, Ping  and
              Wang, Junjie  and
              Gan, Ruyi  and
              Zhu, Xinyu  and
              Zhang, Lin  and
              Wu, Ziwei  and
              Gao, Xinyu  and
              Zhang, Jiaxing  and
              Sakai, Tetsuya",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.474",
    doi = "10.18653/v1/2022.emnlp-main.474",
    pages = "7010--7022",
}

@inproceedings{Costanzino2024MultimodalIA,
  title={Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping},
  author={Alex Costanzino and Pierluigi Zama Ramirez and Giuseppe Lisanti and Luigi Di Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pages={12345-12354},
  doi={10.1109/CVPR2024.1234567}
}

@inproceedings{wu2024minds,
  title={Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models},
  author={Wenshan Wu and Shaoguang Mao and Yadong Zhang and Yan Xia and Li Dong and Lei Cui and Furu Wei},
  booktitle={Adv/lances in Neural Information Processing Systems (NeurIPS)},
  year={2024},
  url={https://arxiv.org/abs/2404.03622}
}

@article{ishay2023leveraging,
    author = {Ishay, Adam and Yang, Zhun and Lee, Joohyung},
    title = {Leveraging Large Language Models to Generate Answer Set Programs},
    journal = {arXiv preprint arXiv:2307.07699},
    year = {2023},
    month = jul,
    eprint = {2307.07699},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI},
    url = {https://arxiv.org/abs/2307.07699},
}

@article{he2023solving,
  title={Solving Math Word Problems by Combining Language Models with Symbolic Solvers},
  author={He-Yueya, Joy and Poesia, Gabriel and Wang, Rose E and Goodman, Noah D},
  journal={arXiv preprint arXiv:2304.09102},
  year={2023}
}

@inproceedings{gao2023pal,
  title     = {{PAL}: Program-Aided Language Models},
  author    = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  volume    = {202},
  pages     = {10764--10799},
  year      = {2023},
  url       = {https://proceedings.mlr.press/v202/gao23f/gao23f.pdf}
}

@inproceedings{feng2024language,
  title     = {Language Models Can Be Deductive Solvers},
  author    = {Feng, J. and Xu, R. and Hao, J. and Sharma, H. and Shen, Y. and Zhao, D. and Chen, W.},
  booktitle = {Findings of the Association for Computational Linguistics: NAACL 2024},
  pages     = {4026--4042},
  year      = {2024}
}

@misc{kalyanpur2024llmarcenhancingllmsautomated,
      title={LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic}, 
      author={Aditya Kalyanpur and Kailash Karthik Saravanakumar and Victor Barres and Jennifer Chu-Carroll and David Melville and David Ferrucci},
      year={2024},
      eprint={2406.17663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17663}, 
}

@inproceedings{hong20233d,
  title={3d concept learning and reasoning from multi-view images},
  author={Hong, Yining and Lin, Chunru and Du, Yilun and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9202--9212},
  year={2023}
}

@inproceedings{gu2024conceptgraphs,
  title={Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning},
  author={Gu, Qiao and Kuwajerwala, Ali and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and others},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5021--5028},
  year={2024},
  organization={IEEE}
}

@inproceedings{majumdar2024openeqa,
  title={Openeqa: Embodied question answering in the era of foundation models},
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16488--16498},
  year={2024}
}

@article{zhang2024countercurate,
  title={Countercurate: Enhancing physical and semantic visio-linguistic compositional reasoning via counterfactual examples},
  author={Zhang, Jianrui and Cai, Mu and Xie, Tengyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2402.13254},
  year={2024}
}

@inproceedings{pratt2023does,
  title={What does a platypus look like? generating customized prompts for zero-shot image classification},
  author={Pratt, Sarah and Covert, Ian and Liu, Rosanne and Farhadi, Ali},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15691--15701},
  year={2023}
}

@inproceedings{alaluf2024myvlm,
  title={Myvlm: Personalizing vlms for user-specific queries},
  author={Alaluf, Yuval and Richardson, Elad and Tulyakov, Sergey and Aberman, Kfir and Cohen-Or, Daniel},
  booktitle={European Conference on Computer Vision},
  pages={73--91},
  year={2024},
  organization={Springer}
}

@article{kuo2022f,
  title={F-vlm: Open-vocabulary object detection upon frozen vision and language models},
  author={Kuo, Weicheng and Cui, Yin and Gu, Xiuye and Piergiovanni, AJ and Angelova, Anelia},
  journal={arXiv preprint arXiv:2209.15639},
  year={2022}
}

@inproceedings{huang2024lita,
  title={Lita: Language instructed temporal-localization assistant},
  author={Huang, De-An and Liao, Shijia and Radhakrishnan, Subhashree and Yin, Hongxu and Molchanov, Pavlo and Yu, Zhiding and Kautz, Jan},
  booktitle={European Conference on Computer Vision},
  pages={202--218},
  year={2024},
  organization={Springer}
}

@article{lv2023kosmos,
  title={Kosmos-2.5: A multimodal literate model},
  author={Lv, Tengchao and Huang, Yupan and Chen, Jingye and Zhao, Yuzhong and Jia, Yilin and Cui, Lei and Ma, Shuming and Chang, Yaoyao and Huang, Shaohan and Wang, Wenhui and others},
  journal={arXiv preprint arXiv:2309.11419},
  year={2023}
}

@article{nasiriany2024pivot,
  title={Pivot: Iterative visual prompting elicits actionable knowledge for vlms},
  author={Nasiriany, Soroush and Xia, Fei and Yu, Wenhao and Xiao, Ted and Liang, Jacky and Dasgupta, Ishita and Xie, Annie and Driess, Danny and Wahid, Ayzaan and Xu, Zhuo and others},
  journal={arXiv preprint arXiv:2402.07872},
  year={2024}
}

@article{konenkov2024vr,
  title={Vr-gpt: Visual language model for intelligent virtual reality applications},
  author={Konenkov, Mikhail and Lykov, Artem and Trinitatova, Daria and Tsetserukou, Dzmitry},
  journal={arXiv preprint arXiv:2405.11537},
  year={2024}
}

@inproceedings{huang2023visual,
  title={Visual language maps for robot navigation},
  author={Huang, Chenguang and Mees, Oier and Zeng, Andy and Burgard, Wolfram},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={10608--10615},
  year={2023},
  organization={IEEE}
}

@inproceedings{gao2024physically,
  title={Physically grounded vision-language models for robotic manipulation},
  author={Gao, Jensen and Sarkar, Bidipta and Xia, Fei and Xiao, Ted and Wu, Jiajun and Ichter, Brian and Majumdar, Anirudha and Sadigh, Dorsa},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={12462--12469},
  year={2024},
  organization={IEEE}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and others},
  year={2023}
}

@inproceedings{fu2024blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  booktitle={European Conference on Computer Vision},
  pages={148--166},
  year={2024},
  organization={Springer}
}

@misc{rahmanzadehgervi2025visionlanguagemodelsblind,
      title={Vision language models are blind: Failing to translate detailed visual features into words}, 
      author={Pooyan Rahmanzadehgervi and Logan Bolton and Mohammad Reza Taesiri and Anh Totti Nguyen},
      year={2025},
      eprint={2407.06581},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.06581}, 
}

@article{gelfond1988stable,
  title={The stable model semantics for logic programming},
  author={Gelfond, Michael and Lifschitz, Vladimir},
  journal={Proceedings of the 5th International Conference on Logic Programming (ICLP/SLP)},
  pages={1070--1080},
  year={1988}
}

@book{gebser2012answer,
  title={Answer Set Solving in Practice},
  author={Gebser, Martin and Kaminski, Roland and Kaufmann, Benjamin and Schaub, Torsten},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}

@book{garcez2002neural,
  title={Neural-Symbolic Learning Systems: Foundations and Applications},
  author={Garcez, Artur S. and Broda, Karolina and Gabbay, Dov M.},
  year={2002},
  publisher={Springer}
}

@inproceedings{sam-abraham-etal-2024-clevr,
    title = "{CLEVR}-{POC}: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
    author = "Sam Abraham, Savitha  and
      Alirezaie, Marjan  and
      de Raedt, Luc",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.293/",
    pages = "3297--3313",
    abstract = "The integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to using existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged in order to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments we observe that the performance of pre-trained vision language models like CLIP (approx. 22{\%}) and a large language model (LLM) like GPT-4 (approx. 46{\%}) on CLEVR-POC are not satisfactory, ascertaining the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC."
}

@inproceedings{lin2014microsoft,
  title={Microsoft {COCO}: Common Objects in Context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={740--755},
  year={2014},
  publisher={Springer},
  address={Zurich, Switzerland},
  doi={10.1007/978-3-319-10602-1_48}
}

@article{krishna2017visual,
  title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and Bernstein, Michael S and Fei-Fei, Li},
  journal={International Journal of Computer Vision},
  volume={123},
  number={1},
  pages={32--73},
  year={2017},
  publisher={Springer},
  doi={10.1007/s11263-016-0981-7},
  url={https://link.springer.com/article/10.1007/s11263-016-0981-7}
}

@inproceedings{zhu2016visual7w,
  title={Visual7W: Grounded Question Answering in Images},
  author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={4995--5004},
  year={2016},
  doi={10.1109/CVPR.2016.539},
  publisher={IEEE}
}

@inproceedings{johnson2017clevr,
  title={{CLEVR}: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2901--2910},
  year={2017},
  publisher={IEEE},
  doi={10.1109/CVPR.2017.322}
}

@inproceedings{hudson2019gqa,
  title={{GQA}: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  author={Hudson, Drew A. and Manning, Christopher D.},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6700--6709},
  year={2019},
  publisher={IEEE},
  doi={10.1109/CVPR.2019.00686}
}

@article{ernst2022ai,
  title={Ai-driven development is here: Should you worry?},
  author={Ernst, Neil A and Bavota, Gabriele},
  journal={IEEE Software},
  volume={39},
  number={2},
  pages={106--110},
  year={2022},
  publisher={IEEE}
}

@article{peng2023impact,
  title={The impact of ai on developer productivity: Evidence from github copilot},
  author={Peng, Sida and Kalliamvakou, Eirini and Cihon, Peter and Demirer, Mert},
  journal={arXiv preprint arXiv:2302.06590},
  year={2023}
}

@article{dakhel2023github,
  title={Github copilot ai pair programmer: Asset or liability?},
  author={Dakhel, Arghavan Moradi and Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse and Desmarais, Michel C and Jiang, Zhen Ming Jack},
  journal={Journal of Systems and Software},
  volume={203},
  pages={111734},
  year={2023},
  publisher={Elsevier}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN international symposium on machine programming},
  pages={1--10},
  year={2022}
}

@article{wang2023codet5+,
  title={Codet5+: Open code large language models for code understanding and generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023}
}

@article{ma2024llamoco,
  title={LLaMoCo: Instruction tuning of large language models for optimization code generation},
  author={Ma, Zeyuan and Guo, Hongshu and Chen, Jiacheng and Peng, Guojun and Cao, Zhiguang and Ma, Yining and Gong, Yue-Jiao},
  journal={arXiv preprint arXiv:2403.01131},
  year={2024}
}

@inproceedings{erdem2009transforming,
  title={Transforming controlled natural language biomedical queries into answer set programs},
  author={Erdem, Esra and Yeniterzi, Reyyan},
  booktitle={Proceedings of the BioNLP 2009 Workshop},
  pages={117--124},
  year={2009}
}

@inproceedings{fang2017approach,
  title={An approach for representing answer sets in natural language},
  author={Fang, Min and Tompits, Hans},
  booktitle={International Workshop on Functional and Constraint Logic Programming},
  pages={115--131},
  year={2017},
  organization={Springer}
}

@article{schwitter2018specifying,
  title={Specifying and verbalising answer set programs in controlled natural language},
  author={Schwitter, Rolf},
  journal={Theory and Practice of Logic Programming},
  volume={18},
  number={3-4},
  pages={691--705},
  year={2018},
  publisher={Cambridge University Press}
}

@article{caruso2024cnl2asp,
  title={CNL2ASP: converting controlled natural language sentences into ASP},
  author={Caruso, Simone and Dodaro, Carmine and Maratea, Marco and Mochi, Marco and Riccio, Francesco},
  journal={Theory and Practice of Logic Programming},
  volume={24},
  number={2},
  pages={196--226},
  year={2024},
  publisher={Cambridge University Press}
}

@inproceedings{baral2012solving,
  title={Solving Puzzles Described in English by Automated Translation to Answer Set Programming and Learning How To Do That Translation.},
  author={Baral, Chitta and Dzifcak, Juraj},
  booktitle={KR},
  year={2012}
}

@article{kuhn2014survey,
  title={A survey and classification of controlled natural languages},
  author={Kuhn, Tobias},
  journal={Computational linguistics},
  volume={40},
  number={1},
  pages={121--170},
  year={2014},
  publisher={Mit press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{nye2021improving,
  title={Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning},
  author={Nye, Maxwell and Tessler, Michael and Tenenbaum, Josh and Lake, Brenden M},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25192--25204},
  year={2021}
}

@article{yang2023coupling,
  title={Coupling large language models with logic programming for robust and general reasoning from text},
  author={Yang, Zhun and Ishay, Adam and Lee, Joohyung},
  journal={arXiv preprint arXiv:2307.07696},
  year={2023}
}

@inproceedings{mitra2016addressing,
  title={Addressing a question answering challenge by combining statistical methods with inductive rule learning and reasoning},
  author={Mitra, Arindam and Baral, Chitta},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{rajasekharan2023reliable,
  title={Reliable natural language understanding with large language models and answer set programming},
  author={Rajasekharan, Abhiramon and Zeng, Yankai and Padalkar, Parth and Gupta, Gopal},
  journal={arXiv preprint arXiv:2302.03780},
  year={2023}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{kafle2017analysis,
  title={An analysis of visual question answering algorithms},
  author={Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1965--1973},
  year={2017}
}

@inproceedings{zhang2016yin,
  title={Yin and yang: Balancing and answering binary visual questions},
  author={Zhang, Peng and Goyal, Yash and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5014--5022},
  year={2016}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={39--48},
  year={2016}
}

@article{agrawal2017c,
  title={C-vqa: A compositional split of the visual question answering (vqa) v1. 0 dataset},
  author={Agrawal, Aishwarya and Kembhavi, Aniruddha and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1704.08243},
  year={2017}
}

@inproceedings{agrawal2018don,
  title={Don't just assume; look and answer: Overcoming priors for visual question answering},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4971--4980},
  year={2018}
}

@inproceedings{li2018vqa,
  title={Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions},
  author={Li, Qing and Tao, Qingyi and Joty, Shafiq and Cai, Jianfei and Luo, Jiebo},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={552--567},
  year={2018}
}

@inproceedings{park2018multimodal,
  title={Multimodal explanations: Justifying decisions and pointing to the evidence},
  author={Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Anna and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8779--8788},
  year={2018}
}

@article{das2017human,
  title={Human attention in visual question answering: Do humans and deep networks look at the same regions?},
  author={Das, Abhishek and Agrawal, Harsh and Zitnick, Larry and Parikh, Devi and Batra, Dhruv},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={90--100},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{ray2018make,
  title={Make up your mind: Towards consistent answer predictions in vqa models},
  author={Ray, Arijit and Burachas, Giedrius T and Sikka, Karan and Roy, Anirban and Ziskind, Avi and Yao, Yi and Divakaran, Ajay},
  booktitle={European Conference on Computer Vision (ECCV), Workshops},
  year={2018}
}

@article{trott2017interpretable,
  title={Interpretable counting for visual question answering},
  author={Trott, Alexander and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1712.08697},
  year={2017}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@inproceedings{biten2019scene,
  title={Scene text visual question answering},
  author={Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4291--4301},
  year={2019}
}

@inproceedings{acharya2019tallyqa,
  title={Tallyqa: Answering complex counting questions},
  author={Acharya, Manoj and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={8076--8084},
  year={2019}
}

@article{wang2017fvqa,
  title={Fvqa: Fact-based visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={10},
  pages={2413--2427},
  year={2017},
  publisher={IEEE}
}

@article{wang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages={3195--3204},
  year={2019}
}

@article{wang2024dspy,
  title={Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs},
  author={Wang, Rong and Sun, Kun and Kuhn, Jonas},
  journal={arXiv preprint arXiv:2411.18564},
  year={2024}
}

@misc{yi2019neuralsymbolicvqadisentanglingreasoning,
      title={Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding}, 
      author={Kexin Yi and Jiajun Wu and Chuang Gan and Antonio Torralba and Pushmeet Kohli and Joshua B. Tenenbaum},
      year={2019},
      eprint={1810.02338},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1810.02338}, 
}

@inproceedings{yang2020neurasp,
  title={NeurASP: Embracing Neural Networks into Answer Set Programming},
  author={Yang, Z. and Ishay, A. and Lee, J.},
  booktitle={Proceedings of the 29th International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2020}
}

@misc{amizadeh2020neurosymbolicvisualreasoningdisentangling,
      title={Neuro-Symbolic Visual Reasoning: Disentangling "Visual" from "Reasoning"}, 
      author={Saeed Amizadeh and Hamid Palangi and Oleksandr Polozov and Yichen Huang and Kazuhito Koishida},
      year={2020},
      eprint={2006.11524},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.11524}, 
}

@inproceedings{bauer2023neuro,
  title={Neuro-symbolic visual graph question answering with LLMs for language parsing},
  author={Bauer, Jakob Johannes and Eiter, Thomas and Ruiz, Nelson Higuera and Oetsch, Johannes},
  booktitle={Workshop on Trends and Applications of Answer Set Programming (TAASP 2023)},
  year={2023}
}

@article{10.1145/182.358434,
author = {Allen, James F.},
title = {Maintaining knowledge about temporal intervals},
year = {1983},
issue_date = {Nov. 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/182.358434},
doi = {10.1145/182.358434},
journal = {Commun. ACM},
month = nov,
pages = {832–843},
numpages = {12},
keywords = {temporal interval, interval representation, interval reasoning}
}

@misc{yang2018graphrcnnscenegraph,
      title={Graph R-CNN for Scene Graph Generation}, 
      author={Jianwei Yang and Jiasen Lu and Stefan Lee and Dhruv Batra and Devi Parikh},
      year={2018},
      eprint={1808.00191},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1808.00191}, 
}

@inproceedings{xu2017scene,
  title     = {Scene Graph Generation by Iterative Message Passing},
  author    = {Xu, Danfei and Zhu, Yuke and Choy, Christopher B. and Fei-Fei, Li},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2017},
  pages     = {5410-5419},
  doi       = {10.1109/CVPR.2017.574},
  url       = {https://arxiv.org/abs/1701.02426}
}

@misc{chen2024spatialvlmendowingvisionlanguagemodels,
      title={SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities}, 
      author={Boyuan Chen and Zhuo Xu and Sean Kirmani and Brian Ichter and Danny Driess and Pete Florence and Dorsa Sadigh and Leonidas Guibas and Fei Xia},
      year={2024},
      eprint={2401.12168},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.12168}, 
}

@misc{cheng2024spatialrgptgroundedspatialreasoning,
      title={SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models}, 
      author={An-Chieh Cheng and Hongxu Yin and Yang Fu and Qiushan Guo and Ruihan Yang and Jan Kautz and Xiaolong Wang and Sifei Liu},
      year={2024},
      eprint={2406.01584},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.01584}, 
}

@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Lempitsky, Michael and Rohrbach, Marcus and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6077--6086},
  year={2018}
}

@inproceedings{lu2016hierarchical,
  title={Hierarchical question-image co-attention for visual question answering},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  booktitle={Advances in neural information processing systems},
  pages={289--297},
  year={2016}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining for grounding vision and language},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in neural information processing systems},
  pages={13--23},
  year={2019}
}

@inproceedings{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)},
  pages={5100--5111},
  year={2019}
}

@inproceedings{su2019vl,
  title={Vl-bert: Pre-training of generic visual-linguistic representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Gao, Pengchuan and Zhang, Xiaowei and He, Xiaodong and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and Choi, Yejin and Gao, Jianfeng},
  booktitle={European Conference on Computer Vision},
  pages={121--137},
  year={2020},
  organization={Springer}
}

@misc{openai2023gpt4v,
    title={GPT-4V(ision) System Card},
    author={OpenAI},
    year={2023},
    howpublished={\url{https://cdn.openai.com/papers/GPTV_System_Card.pdf}}
}

@misc{ge2021selfdistillationbatchknowledgeensembling,
      title={Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification}, 
      author={Yixiao Ge and Xiao Zhang and Ching Lam Choi and Ka Chun Cheung and Peipei Zhao and Feng Zhu and Xiaogang Wang and Rui Zhao and Hongsheng Li},
      year={2021},
      eprint={2104.13298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.13298}, 
}

@misc{liang2022visualabductivereasoning,
      title={Visual Abductive Reasoning}, 
      author={Chen Liang and Wenguan Wang and Tianfei Zhou and Yi Yang},
      year={2022},
      eprint={2203.14040},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2203.14040}, 
}

@inproceedings{hogg1983block,
  title={Block Worlds},
  author={Hogg, David C.},
  booktitle={AAAI Spring Symposium on Physical Agents},
  year={1983}
}

@misc{campbell2024understandinglimitsvisionlanguage,
      title={Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem}, 
      author={Declan Campbell and Sunayana Rane and Tyler Giallanza and Nicolò De Sabbata and Kia Ghods and Amogh Joshi and Alexander Ku and Steven M. Frankland and Thomas L. Griffiths and Jonathan D. Cohen and Taylor W. Webb},
      year={2024},
      eprint={2411.00238},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.00238}, 
}

@misc{anis2025limitationsvisionlanguagemodelsunderstanding,
      title={On the Limitations of Vision-Language Models in Understanding Image Transforms}, 
      author={Ahmad Mustafa Anis and Hasnain Ali and Saquib Sarfraz},
      year={2025},
      eprint={2503.09837},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.09837}, 
}

@misc{vardi2025clipupclipbasedunanswerableproblem,
      title={CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering}, 
      author={Ben Vardi and Oron Nir and Ariel Shamir},
      year={2025},
      eprint={2501.01371},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.01371}, 
}

@misc{borroto2024automaticcompositionaspprograms,
      title={Towards Automatic Composition of ASP Programs from Natural Language Specifications}, 
      author={Manuel Borroto and Irfan Kareem and Francesco Ricca},
      year={2024},
      eprint={2403.04541},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.04541}, 
}

@misc{chiyahgarcia2024repairsblockworldnew,
      title={Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models}, 
      author={Javier Chiyah-Garcia and Alessandro Suglia and Arash Eshghi},
      year={2024},
      eprint={2409.14247},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.14247}, 
}

@misc{silver2023generalizedplanningpddldomains,
      title={Generalized Planning in PDDL Domains with Pretrained Large Language Models}, 
      author={Tom Silver and Soham Dan and Kavitha Srinivas and Joshua B. Tenenbaum and Leslie Pack Kaelbling and Michael Katz},
      year={2023},
      eprint={2305.11014},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2305.11014}, 
}

@article{Hennekeuser_etal_2024,
  author    = {Hennekeuser, Tim and Ihme, Tobias Alexander and Rensing, Christoph},
  title     = {The Impact of Artificial Intelligence on Students' Academic Development},
  journal   = {Frontiers in Education},
  volume    = {9},
  year      = {2024},
  doi       = {10.3389/feduc.2024.1392091},
  url       = {https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1392091/full}
}

@misc{moe_ai_2024,
  title={教育部部署加强中小学人工智能教育},
  author={中华人民共和国教育部},
  year={2024},
  month={12},
  day={02},
  type={新闻},
  url={http://www.moe.gov.cn/jyb_xwfb/gzdt_gzdt/s5987/202412/t20241202_1165500.html}
}

@inproceedings{li2022grounded,
  title     = {Grounded Language-Image Pre-training},
  author    = {Li, Jiasen and Li, Nan and Savarese, Silvio and Niebles, Juan Carlos and Fei-Fei, Li and Yu, Donglai},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2022},
  pages     = {10965--10975}
}

@misc{bortolotti2024neurosymbolicbenchmarksuiteconcept,
      title={A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts}, 
      author={Samuele Bortolotti and Emanuele Marconato and Tommaso Carraro and Paolo Morettin and Emile van Krieken and Antonio Vergari and Stefano Teso and Andrea Passerini},
      year={2024},
      eprint={2406.10368},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.10368}, 
}

@misc{sclar2024quantifyinglanguagemodelssensitivity,
      title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting}, 
      author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
      year={2024},
      eprint={2310.11324},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11324}, 
}

@misc{gao2025multimodalagenttuningbuilding,
      title={Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage}, 
      author={Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaojian Ma and Tao Yuan and Yue Fan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li},
      year={2025},
      eprint={2412.15606},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.15606}, 
}

@book{10.5555/3073924,
author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
title = {Automated Planning and Acting},
year = {2016},
isbn = {1107037271},
publisher = {Cambridge University Press},
address = {USA},
edition = {1st},
abstract = {Autonomous AI systems need complex computational techniques for planning and performing actions. Planning and acting require significant deliberation because an intelligent system must coordinate and integrate these activities in order to act effectively in the real world. This book presents a comprehensive paradigm of planning and acting using the most recent and advanced automated-planning techniques. It explains the computational deliberation capabilities that allow an actor, whether physical or virtual, to reason about its actions, choose them, organize them purposefully, and act deliberately to achieve an objective. Useful for students, practitioners, and researchers, this book covers state-of-the-art planning techniques, acting techniques, and their integration which will allow readers to design intelligent systems that are able to act effectively in the real world.}
}

@misc{cunnington2024neurosymboliclearninganswerset,
      title={Neuro-Symbolic Learning of Answer Set Programs from Raw Data}, 
      author={Daniel Cunnington and Mark Law and Jorge Lobo and Alessandra Russo},
      year={2024},
      eprint={2205.12735},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2205.12735}, 
}

@misc{yang2023neuraspembracingneuralnetworks,
      title={NeurASP: Embracing Neural Networks into Answer Set Programming}, 
      author={Zhun Yang and Adam Ishay and Joohyung Lee},
      year={2023},
      eprint={2307.07700},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.07700}, 
}

@misc{fang2024largelanguagemodelsneurosymbolic,
      title={Large Language Models Are Neurosymbolic Reasoners}, 
      author={Meng Fang and Shilong Deng and Yudi Zhang and Zijing Shi and Ling Chen and Mykola Pechenizkiy and Jun Wang},
      year={2024},
      eprint={2401.09334},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.09334}, 
}

@misc{premsri2025neurosymbolictrainingreasoningspatial,
      title={Neuro-symbolic Training for Reasoning over Spatial Language}, 
      author={Tanawan Premsri and Parisa Kordjamshidi},
      year={2025},
      eprint={2406.13828},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13828}, 
}

@misc{andreas2017neuralmodulenetworks,
      title={Neural Module Networks}, 
      author={Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Dan Klein},
      year={2017},
      eprint={1511.02799},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1511.02799}, 
}

@misc{hu2017learningreasonendtoendmodule,
      title={Learning to Reason: End-to-End Module Networks for Visual Question Answering}, 
      author={Ronghang Hu and Jacob Andreas and Marcus Rohrbach and Trevor Darrell and Kate Saenko},
      year={2017},
      eprint={1704.05526},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1704.05526}, 
}

@misc{mao2019neurosymbolicconceptlearnerinterpreting,
      title={The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision}, 
      author={Jiayuan Mao and Chuang Gan and Pushmeet Kohli and Joshua B. Tenenbaum and Jiajun Wu},
      year={2019},
      eprint={1904.12584},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1904.12584}, 
}

@article{bhargava2022commonsense,
  title={Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey},
  author={Bhargava, Prajjwal and Ng, Vincent},
  journal={arXiv preprint arXiv:2201.12438},
  year={2022}
}

@article{rajani2019explain,
  title={Explain Yourself! Leveraging Language Models for Commonsense Reasoning},
  author={Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1906.02361},
  year={2019}
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{wei2022chain,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and ... and Le, Quoc V.},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@misc{fandinno2021planningincompleteinformationquantified,
      title={Planning with Incomplete Information in Quantified Answer Set Programming}, 
      author={Jorge Fandinno and François Laferrière and Javier Romero and Torsten Schaub and Tran Cao Son},
      year={2021},
      eprint={2108.06405},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2108.06405}, 
}