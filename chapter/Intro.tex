\chapter{绪论}
\section{研究背景}
随着计算机视觉（Computer Vision）
和自然语言处理（Natural Language Processing）技术的迅猛发展，
跨模态智能理解成为人工智能研究的重要方向之一。
其中，视觉问答（Visual Question Answering, VQA）\cite{goyal2017making}任务因其广泛的应用前景和挑战性，
受到了学术界和工业界的广泛关注。例如，给定一幅包含动物的图片，
系统需要能够回答“这只动物是什么颜色？”或“图片中有几只猫？”等问题。
这一任务的核心在于多模态信息的深度融合，即如何在视觉特征和语言信息之间建立有效的联系。
积木世界\cite{hogg1983block}是人工智能研究、教学、实验、评估的重要场景，能够模拟实际应用中物体间的空间关系。
研究积木世界VQA任务，能够有效考察智能体在空间推理方面的能力。多年以来，有多种方法被提出用于解决VQA任务，其中
视觉语言模型（Visual Language Model, VLM）因其强大的多模态理解能力而备受关注。

VLM是一种多模态模型，能够同时处理图像和文本信息，并生成与图像内容相关的自然语言响应。
如图\ref{fig:vlm-example}所示为VLM的一个简单流程示例。
VLM通过将视觉编码器与大语言模型结合，赋予模型“看”与“理解”的能力。与传统的计算机视觉模型不同，VLM 不受固定
类别集或特定任务 (如分类或检测) 约束。在大量文本和图像/视频字幕对的语料上进行重新训练，
VLM 可以用自然语言进行指导，并用于处理许多典型的视觉任务以及新的生成式 AI 任务，
例如摘要和视觉问答。如图\ref{fig:vlm-architecture}所示为VLM的通用架构，包括视觉编码器、投影器和大语言模型（Large Language Model, LLM）三个部分。
视觉编码器（如CLIP模型）具有图像与文本的关联能力，负责提取图像特征。投影器由一组网络层构成，负责将视觉特征转换为LLM可理解的标记（Token）。
LLM负责生成文本输出，支持对话、推理等任务，目前任何现有的LLM（如ChatGPT、Llama、DeepSeek等）都可以用来构建VLM。
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/VLM-example.png}
    \caption{视觉语言模型用例}
    \label{fig:vlm-example}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[scale=0.6]{figures/vlm-architecture-diagram-crop.pdf}
    \caption{视觉语言模型的通用三部分架构}
    \label{fig:vlm-architecture}
\end{figure}

尽管VLM在图像分类\cite{pratt2023does}、字幕生成\cite{alaluf2024myvlm}、
目标检测\cite{kuo2022f}、视频理解\cite{huang2024lita}和文档解析\cite{lv2023kosmos}
等各种任务中取得了良好效果，但VLM仍难以有效应对部分可见场景中基于空间推理的VQA任务。
部分可见指的是在图像中，某些物体或其特征由于遮挡、模糊、视野受限、噪声或其他原因而不可见的情况，
导致模型无法从图像中获得解决问题所需的全部信息。
现有的VLM在部分可见场景中解决VQA任务时，可能会出现幻觉\cite{vardi2025clipupclipbasedunanswerableproblem}
、空间关系推理能力下降\cite{chen2024spatialvlmendowingvisionlanguagemodels}\cite{anis2025limitationsvisionlanguagemodelsunderstanding}
，并且在对象识别和计数任务中表现不佳\cite{campbell2024understandinglimitsvisionlanguage}，不能真正理解图像信息\cite{rahmanzadehgervi2025visionlanguagemodelsblind}等问题。
这类部分可见场景在现实世界中广泛存在，例如在机器人抓取、自动驾驶、安防监控等实际应用中，
系统往往需要在信息不完全的条件下对场景作出准确理解和决策。

因此，研究能够在部分可见场景中进行准确空间推理的VQA方法，不仅具有理论挑战性，也具有重要的工程价值。
一方面，它有助于提升人工智能系统在复杂环境中的稳健性和泛化能力；
另一方面，也为智能机器人\cite{gao2024physically}\cite{nasiriany2024pivot}、
AR/VR交互系统\cite{konenkov2024vr}等场景中的感知与决策提供了新的技术路径。

为了解决多模态模型在空间推理方面的问题，研究者们提出了一些新的方法。近年来，
神经符号方法（Neuro-Symbolic Methods）得到了广泛关注。在多模态空间推理场景中，
神经符号方法使用深度学习技术进行感知，为输入的图像和问题分别生成符号表示，再使用符号系统进行推理求解，
可以有效提升多模态模型在空间推理方面的能力。

在神经符号方法中，神经网络的选择十分多样，通常包括卷积神经网络（Convolution Neural Network, CNN）、
循环神经网络（Recurrent Neural Network, RNN）及其变体、图神经网络（Graph Neural Network, GNN）和
Transformer架构及LLM。相比其它神经网络，LLM用于神经符号系统，具有以下突出优势：
\begin{enumerate}[itemsep=0pt,parsep=0pt]
    \item 丰富的预训练知识库。LLM在大规模语料上进行预训练，具备广泛的世界知识和语言理解能力，这使得它们在处理涉及常识推理和多领域知识整合时，比专门针对单一任务训练的网络更具优势。
    \item 自然语言理解与生成能力。LLM能够将符号系统产生的抽象逻辑或规则，用自然语言进行解释和反馈，从而降低了人机交互的门槛，并使得系统的推理过程更易于理解和验证。
    \item 零样本/少样本学习能力。由于在大规模数据上学习到了通用的语言模式，LLM在面对新领域或数据较少的任务时，往往能够直接发挥作用，从而提高神经符号系统的泛化能力。
    \item 灵活的上下文建模。LLM能够捕捉长距离依赖和上下文信息，这对于将符号系统中离散的知识点进行有机整合，以及在复杂场景中实现动态推理具有重要意义。
\end{enumerate}
结合以上各项理由，将LLM作为神经符号系统中的神经模块，能够与符号系统形成互补，既利用符号逻辑的严谨性，又发挥神经网络在语义理解和自然语言生成方面的优势，从而构建更强大、更灵活的智能系统。

在神经符号方法中，符号系统的选择同样多种多样，例如知识图谱嵌入（Knowledge Graph Embeddings, KGE）、逻辑神经网络（Logical
 Neural Networks, LNN）都为信息表示和推理提供了新思路。然而，在当前的研究和应用中，回答集编程（Answer Set Programming，ASP）往往被更多
地采用作为符号系统。ASP是一种声明式编程范式，可用于解决复杂的人工智能问题，其起源于对逻辑编程、非单调推理和知识表示的研究。
相比其它符号系统，ASP被更多用于神经符号方法中，主要是由于以下几点优势：（1）明确的逻辑语义与可解释性。ASP基于严格定义的逻辑规则，其推理过程透明、易于解释。这一特性在需要精确推理和结果可验证的任务中尤为重要\cite{gelfond1988stable}；
（2）强大的非单调推理能力。ASP自然支持非单调逻辑推理，能够处理默认情况和异常情形，这使其在面对现实世界中不完全或动态变化的信息时表现出色\cite{gelfond1988stable}；
（3）成熟的求解器与工具支持。像Clingo这样的ASP求解器已经经过长期验证，具有高效、稳定的特点，能够处理大规模问题，这为实际应用提供了有力保障\cite{gebser2012answer}；
（4）便于与神经网络等模块整合。ASP的声明式表达方式使其易于与神经网络输出的信息对接，形成互补优势，从而提升整体系统的推理能力\cite{garcez2002neural}。
ASP的这些特性使得它作为符号系统的杰出代表，被广泛应用于神经符号方法中。

尽管神经网络采用了预训练的LLM，而符号系统部分选用了ASP，但将二者有效结合在一起仍然是一项复杂的任务。
为此，近年来出现了一些神经符号框架，试图通过模块化设计和统一接口来简化LLM与ASP的集成开发流程\cite{wang2024dspybasedneuralsymbolicpipelineenhance}。
尽管现有的框架为LLM结合ASP的开发过程提供了诸多便利，降低了使用门槛，然而其在ASP规则的自动拓展方面存在不足，
需要依赖人工干预进行规则扩充，在这一方面仍存在改进空间。

为了测试模型解决空间推理问题的能力，涌现了诸如CLEVR、GQA、COCO等数据集。
这些数据集由问题答案对（Question and Answer Pair，以下简称QA对）组成，
并且包含问题对应的场景图像。其中，CLEVR数据集因高度结构化的场景构造、精确标注的物体属性及复杂的空间关系，
而被广泛用于人工智能的研究和课堂教学等重要场景。
然而，CLEVR数据集图像均基于完整场景生成，所有回答问题的信息都直接可见，缺乏现实环境中常见的部分可见性，
难以有效考察VLM在部分可见场景中的推理能力\cite{sam-abraham-etal-2024-clevr}。

基于上述背景，本文重点关注构建部分可见场景空间推理VQA数据集，
并对现有的神经符号VQA框架进行改进，并设计实现一个VQA课堂教学演示原型系统，为开展人工智能的基础教学提供支持。
\section{相关研究现状}
基于以上背景，本节主要从视觉问答的发展与现状、空间推理的发展与现状、
视觉问答数据集的发展与现状、生成ASP程序的发展与现状、神经符号方法在空间推理中的发展与现状来进行综述。

\subsection{视觉问答的发展与现状}
VQA是由Antol等人\cite{Antol2015VQA}提出的，是人工智能领域中一个富有挑战性的交叉研究方向，
涉及计算机视觉（CV）、自然语言处理（NLP）和机器学习（ML）等多个学科。
其核心任务是让机器能够根据给定的图像内容和相关的自然语言问题，生成准确的自然语言答案。
VQA不仅要求模型理解图像中的视觉信息（如物体、属性、关系）和问题的语义意图，还需要进行复杂的推理，
融合这两种模态的信息来找到或生成答案。近年来，随着深度学习技术的发展，VQA领域取得了显著进展，但也面临着诸多挑战。

早期的VQA模型通常采用相对简单的框架，例如使用卷积神经网络（CNN）提取图像特征，使用循环神经网络（RNN）
或长短期记忆网络（LSTM）编码问题，然后通过简单的融合策略（如元素积、拼接）结合两种模态的特征，最后输入到一个分类器中预测答案。

然而，这类方法难以有效关联问题中的关键信息和图像中的特定区域。为了解决这个问题，注意力机制
被广泛引入VQA模型中。其核心思想是让模型根据问题的指导，动态地聚焦于图像中最相关的区域。其中， Anderson等人提出的自下而上的注意力机制\cite{anderson2018bottom}
成为后续许多模型的基础。该方法首先使用Faster R-CNN等目标检测器提取图像中的显著区域，
然后根据问题生成一个查询向量，计算该查询与各个区域特征的相似度（注意力权重），最后对区域特征进行加权求和，
得到与问题高度相关的视觉特征。这种方法显著提升了模型性能，因为它能更精确地定位与问题相关的视觉证据。
后续研究进一步发展了注意力机制，如分层注意力\cite{lu2016hierarchical}和共同注意力，后者允许模型同时学习图像区域和问题词语之间的相互关联，从而实现更深层次的跨模态信息交互。

随着Transformer架构在NLP领域的巨大成功，研究者们也开始将其应用于VQA任务，以更好地捕捉长距离依赖和模态间的复杂交互。
这些模型通常利用Transformer的自注意力和交叉注意力机制。ViLBERT\cite{lu2019vilbert}和 LXMERT\cite{tan2019lxmert}是早期的代表性模型。
它们采用双流架构，分别处理视觉和文本输入，并通过跨模态Transformer层进行信息交互和融合。VL-BERT\cite{su2019vl}和 Oscar\cite{li2020oscar}则采用了单流架构，
将图像区域特征和文本词嵌入拼接在一起，输入到一个统一的Transformer编码器中进行联合建模。Oscar还引入了目标标签作为“锚点”，
连接图像区域和对应的文本描述，增强了跨模态对齐。

这些基于Transformer的视觉语言预训练（Vision-Language Pre-training, VLP）模型通常在大规模的图像-文本对数据上进行预训练，
学习通用的跨模态表示，然后在特定的VQA数据集上进行微调，取得了显著的性能提升。

近年来，LLM如GPT-3、PaLM等在自然语言处理任务上展现了惊人的能力。
研究者们迅速探索了将LLM应用于VQA领域，催生了大型多模态模型（Large Multimodal Model, LMM） 或 
VLM的研究热潮。这类模型通常利用预训练好的强大LLM作为基础，通过设计特定的接口或适配器将视觉信息“注入”到LLM中，使其能够理解图像内容并回答相关问题。
OpenAI发布的GPT-4V\cite{openai2023gpt4v}展示了前所未有的多模态理解和推理能力，
能够处理复杂的视觉问答、图像描述、图表解读等任务。虽然其具体架构和训练细节未完全公开，但它代表了当前LMM能力的顶尖水平。

基于LLM的VQA模型通常具有更强的零样本/少样本泛化能力、更自然的语言交互能力、更好的常识推理能力，
并且能够处理更开放式、更复杂的查询。它们将VQA从一个多模态分类/生成任务，转变为一个基于视觉信息的条件语言生成任务。

尽管VQA领域取得了巨大进步，但仍面临一些挑战。
第一是深度推理与组合性。模型在处理需要多步逻辑推理、精确计数或复杂空间关系的问题时仍然困难。
第二是鲁棒性与偏见。模型容易受到对抗性样本攻击，并且可能学习到数据集中的统计偏见，而非真正的视觉理解。
第三是可解释性。理解模型为何给出特定答案仍然是一个挑战，尤其对于复杂的黑箱模型。
第四是知识依赖。如何更有效地融合和利用外部世界知识或领域知识来回答专业性或常识性问题。
第五是效率与部署。大型多模态模型通常计算量巨大，如何在资源受限的设备上高效部署是一个实际问题。

\subsection{视觉问答数据集的发展与现状}
VQA数据集可以分为四类：通用数据集、合成数据集、诊断数据集、基于知识的数据集。通用数据集是由自然图像和人工生成的问答对组成的大规模数据集。
合成数据集包含计算机生成的图像，其问题通过固定数量的手工模板构建而成。诊断数据集是较小规模的数据集，用于单独评估模型的特定能力。
最后，基于知识的数据集提供可查询额外信息的外部知识库。
\subsubsection{通用数据集}
通用数据集是视觉问答（VQA）领域中规模最大、内容最丰富且应用最广泛的数据集类型。此类数据集通常包含来
自主流图像数据库（如MS-COCO\cite{lin2014microsoft}和ImageNet\cite{deng2009imagenet}）的数万幅真实场景图像，具有显著的规模优势与多样性特征
。尽管这些数据集尚无法完全覆盖真实世界视觉元素的无限复杂性，但其在广泛性和通用性方面已取得较好近似。

COCO-QA是早期VQA数据集之一，基于MS-COCO的12.3万幅图像构建。其问答对通过图像描述自动生成，主要包含对象存在性、数量、颜色及位置等基础问题类型，问题复杂度较低且类别有限。

VQA-v1\cite{Antol2015VQA}（常简称为"VQA数据集"）是首个大规模VQA数据集，
包含真实场景（VQA-real）与抽象场景（VQA-abstract）两个子集。真实子集涵盖多目标复杂场景，
抽象子集则由剪贴画软件生成简化场景（合成数据集部分将详细讨论）。该数据集通过亚马逊众包平台（AMT）收集问题与答案，要求工作者提出"能难倒智能机器人"的问题。然而，数据集存在显著偏差：55.86%的二元判断题答案为"是"，数字类问题中"2"占比最高[10]。此外，部分问题具有主观性与观点性，难以客观评估答案正确性，且人工标注一致性较低。

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{figures/example-from-vqa-v1.jpg}
    \begin{center}
        \footnotesize 问题：图片中有多少片披萨？\\
        \footnotesize 问题：这是不是素的披萨？
    \end{center}
    \caption{VQA-v1数据集的示例}
    \label{fig:example-from-vqa-v1}
\end{figure}

VQA-v2\cite{goyal2017making}（平衡版VQA）针对VQA-v1的偏差问题进行优化设计。
研究团队发现原始数据集存在严重的语言先验偏差，导致模型可通过利用统计规律而非图像内容提升准确率。
为此，VQA-v2为每个问题提供两幅语义相似但答案不同的图像，强制模型必须依赖视觉信息进行推理。

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth, keepaspectratio]{figures/hydrant-a.png}
        \caption*{\footnotesize 问题：消防栓是什么颜色？\\答案：红色}
        \label{fig:example-from-vqa-v2-1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth, keepaspectratio]{figures/hydrant-b.png}
        \caption*{\footnotesize 问题：消防栓是什么颜色？\\答案：黑色}
        \label{fig:example-from-vqa-v2-2}
    \end{subfigure}
    \caption{来自VQA-v2数据集的、针对问题‘消防栓是什么颜色？’的两张互补图像（答案不同）}
    \label{fig:example-from-vqa-v2}
\end{figure}

Visual Genome, VG​\cite{krishna2017visual}是信息标注最丰富的VQA数据集，包含10万幅图像，平均每幅图像标注21个对象、
18个属性及18组对象间关系。该数据集包含七类核心标注：区域描述、对象、属性、关系、区域图、场景图及问答对，首次通过场景图形式提供结构化图像表示。
其问题设计强调明确性与可验证性，所有问题均需严格依赖图像内容作答。与VQA-v1相比，该数据集优化问题类型分布，将二元判断题占比从32.37\%降至零，以鼓励更具挑战性的问答对。

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth, keepaspectratio]{figures/visual-genome-a.png}
        \caption*{(a)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth, keepaspectratio]{figures/visual-genome-b.png}
        \caption*{(b)}
    \end{subfigure}
    \caption{Visual Genome中的示例图像(a)及其对应的一部分场景图(b)}
\end{figure}

Visual7W\cite{zhu2016visual7w}通过对象级语义关联建立文本描述与图像区域的映射关系。
对于问题中涉及对象的词汇，均提供对应图像区域的边界框标注，帮助模型建立问题对象与视觉实体的关联。
该数据集涵盖七类问题（What/Where/When/Who/Why/How/Which），前六类需生成描述性答案，最后一类需指向具体图像区域作为答案，不包含二元判断题类型。

TDIUC\cite{kafle2017analysis}专为消除数据集偏差设计，包含160K图像上的160万个问题，
划分至12个独立类别以评估模型在特定视觉任务上的表现。该数据集通过均衡问题类型分布与答案分布缓解先前模型的评估偏差问题，
例如：在VQA-v1中提升"Is/Are"类问题准确率15\%可使整体准确率提高5\%，而同等幅度的"Why/Where"类问题改进仅带来4.1\%增益。
此外，TDIUC引入异常问题（需回答"不适用"），并采用归一化评估指标以消除答案分布偏斜影响。受限于答案长度约束（多为1-2个词汇），其唯一答案数量约为一千个。

GQA\cite{hudson2019gqa}通过融合组合推理与真实图像构建大规模数据集，包含11.3万幅图像上的2200万个问题。该数据集继承Visual Genome的场景图标注体系，
同时借鉴CLEVR\cite{johnson2017clevr}的问题结构化表示方法，将问题转换为可执行的函数式程序以显式描述推理步骤。
相较于CLEVR，GQA采用真实图像而非合成场景，在对象类别与属性多样性方面更具优势。其问题生成过程结合自然语言语法规则与概率语法模型，
提升问题的自然性与抗语言记忆能力。通过采样平衡策略有效降低问题与答案分布的偏斜，并设计一致性、有效性、合理性等多维度评估指标以全面衡量模型能力。
\subsubsection{合成数据集}
合成数据集是指通过计算机软件生成的虚拟图像数据而非真实场景图像。优质的视觉问答模型应具备与人类相似的跨模态推理能力，既能处理真实图像数据，又能有效应对合成数据
。相较于真实数据采集，合成数据集具有显著优势：其构建过程可通过自动化流水线实现，大幅降低时间成本与经济投入；
同时，研究者可针对性设计复杂推理任务，通过精细化控制场景元素来增强模型组合推理能力的可评估性。

VQA-Abstract作为VQA-v1数据集的子集，其包含50,000个基于剪贴画软件生成的抽象场景。相较于真实场景图像，
此类图像具有结构简单、噪声干扰少的特点，其核心设计目标在于降低底层视觉特征复杂度，迫使模型聚焦于高层语义推理能力的训练。

Yin and Yang\cite{zhang2016yin}。该数据集针对VQA-Abstract存在的语言偏差问题进行优化改进，重点解决二元（是/否）问题的答案分布失衡问题。
通过平衡正负样本比例，并设计互补场景对（即同一问题在不同场景下对应相反答案），有效抑制模型依赖语言先验而非图像内容进行预测的倾向。

SHAPES\cite{andreas2016neural}数据集由64张几何形状排列图构成，每张图像配备244个二元问题，总计生成15,616个问答对。该数据集规模较小，
主要用于评估模型空间关系推理能力。其简洁的几何元素与有限的问题类型使其成为诊断模型基础空间认知能力的有效工具。

CLEVR\cite{johnson2017clevr}是一个专门用于评估模型高层次推理能力的数据集。CLEVR 包含 10 万张合成图像和 100 万个人工生成的问题。
场景由一组带有形状、大小、颜色、材质以及在地面平面上的位置等注释的物体组成。CLEVR 的世界由三种物体形状构成，
每种形状具有两种绝对尺寸、两种材质以及八种颜色。物体之间的空间关系包括四种类型：“左侧”、“右侧”、“后方”和“前方”。CLEVR 中的问题通过手工制作的模板自动生成。

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{figures/clevr-example.png}
    \begin{center}
        \footnotesize 问题1：大物体和金属球的数量是否相等？\\
        \footnotesize 问题2：大球剩下的棕色金属物体的圆柱体尺寸是多少？\\
        \footnotesize 问题3：有一个球体与金属立方体大小相同；它是由与红色小球相同的材料制成的吗？\\
        \footnotesize 问题4：有多少物体是小圆柱体或金属物体？问题：有多少个小的蓝色金属球？\\
        \footnotesize 问题5：在左边的物体是什么颜色？
    \end{center}
    \caption{CLEVR数据集的示例}
    \label{fig:clevr-example}
\end{figure}

CLEVR 的一个独特特点是，它为每个问题提供了结构化的表示，作为额外注释。每个问题被表示为一系列函数，
这些函数反映了解答该问题所需的正确推理步骤。与通用的 VQA 数据集相比，CLEVR 的问题更长、更复杂。这使得 CLEVR 能够生成更多样化的独特问题，
从而减轻问题的条件偏差。较长的问题通常也对应更长的推理过程。

CLEVR 的一个缺点是其图像内容相对简单和基础。由于物体种类和特征较少，模型容易过拟合。
为测试这一问题并验证模型的组合泛化能力，作者合成了两个新的 CLEVR 版本：1）在条件 A 中，所有立方体仅为灰色、蓝色、棕色或黄色，
所有圆柱体仅为红色、绿色、紫色或青色；2）在条件 B 中，这些形状交换颜色配置。两个条件下的球体涵盖全部八种颜色。理想的模型应能够在一个条件下训练，
并在另一个条件下表现良好。由于 CLEVR 不包含真实图像或自然语言问题，作者建议仅将其用作测试模型组合能力的诊断工具。
CLEVR 还提供了一个变体 CLEVR-humans，其中的问题由人类以自然语言提供。
\subsubsection{诊断数据集}
诊断型数据集旨在针对特定能力对模型进行细粒度评估，通常规模较小，用于补充通用数据集的不足，以揭示模型在特定场景下的性能瓶颈。以下为典型诊断型数据集及其特性：
\begin{enumerate}[itemsep=0pt,parsep=0pt]
    \item 组合能力评估数据集。

​C-VQA\cite{agrawal2017c}：基于VQA-v1重构的数据集，通过重组训练集与验证集的划分，测试模型对未见概念组合的推理能力。
例如，在训练集中，“运动类”问题的常见答案为“网球”，而测试集则替换为“滑雪”，以验证模型是否真正理解组合语义而非依赖数据分布偏差。

\begin{figure}[h]
    \centering
    
    % 第一行
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/CVQA-A.png}
        \caption*{问题：盘子的颜色是什么？\\答案：绿色}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/CVQA-B.png}
        \caption*{问题：交通信号灯的颜色是什么？\\答案：红色}
    \end{subfigure}

    {\textbf{训练}}

    \vspace{0.5em}
    % 第二行
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/CVQA-C.png}
        \caption*{问题：交通信号灯的颜色是什么？\\答案：绿色}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/CVQA-D.png}
        \caption*{问题：盘子的颜色是什么？\\答案：红色}
    \end{subfigure}

    {\textbf{测试}}

    \vspace{0.5em}
    \caption{CVQA数据集示例}
    \label{fig:cvqa-example}
\end{figure}

​VQA-CP\cite{agrawal2018don}：通过重构VQA-v1与VQA-v2的问题类型-答案分布，使训练集与测试集的答案分布差异显著，强制模型避免利用数据集偏差进行预测。
    \item 解释生成数据集。

​VQA-E\cite{li2018vqa}：包含27万条自动生成的文本解释，覆盖10.8万张图像，旨在训练模型生成答案的逻辑依据。

​VQA-X\cite{park2018multimodal}：规模较小（4.2万条解释，2.8万张图像），提供人工标注的文本解释与视觉注意力热图，支持多模态解释生成。

​VQA-HAT\cite{das2017human}：通过人工标注的注意力区域（用户需对模糊图像局部去模糊以回答问题），提供视觉注意力真值，用于评估模型注意力机制的合理性。
    \item 语言变体鲁棒性数据集。

​VQA-Rephrasings\cite{shah2019cycle}：针对VQA-v2验证集的4万张图像，为每个问题提供3种人工复述表述，测试模型对语言表述变化的鲁棒性，并提出通用方法以提升复述问题的回答能力。
    \item 逻辑一致性数据集。

​ConVQA\cite{ray2018make}：包含679万条问题对，覆盖10.8万张图像，设计语义一致但表述不同的问题（如“是否为素食披萨？”与“是否含有意大利辣香肠？”），验证模型逻辑推理的一致性。
\begin{figure}[h]
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth, keepaspectratio]{figures/CONVQA-A.png}
        \begin{center}
            \footnotesize 问题1：图中有多少人？\\
            \footnotesize 答案1:1\\
            \footnotesize 问题2：图中是否为1个人？\\
            \footnotesize 答案2：是\\
            \footnotesize 问题3：图中是没有人吗？\\
            \footnotesize 答案3：否\\
            \footnotesize 问题4：图中有6个人吗？\\
            \footnotesize 答案4：否\\
        \end{center}
        \caption*{(a)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth, keepaspectratio]{figures/CONVQA-B.png}
        \begin{center}
            \footnotesize 问题1：图中的人穿的什么？\\
            \footnotesize 答案1:T恤衫\\
            \footnotesize 问题2：这个人是穿的T恤衫吗？\\
            \footnotesize 答案2：是\\
            \footnotesize 问题3：谁在穿着T恤衫？\\
            \footnotesize 答案3：男人\\
            \footnotesize 问题4：窗帘穿的是衬衫吗？\\
            \footnotesize 答案4：否\\
        \end{center}
        \caption*{(b)}
    \end{subfigure}
    \caption{ConVQA数据集的示例}
    \centering
    \label{fig:convqa-example}
\end{figure}
    \item 计数能力数据集。

​HowManyQA\cite{trott2017interpretable}：整合VQA-v2与Visual Genome的计数问题，形成10.6万条简单计数问题（如“图像中有多少只狗？”）。

​TallyQA\cite{acharya2019tallyqa}：扩展复杂计数场景，包含28.8万条问题（其中7.6万条为复杂问题），需结合对象属性与关系推理（如“草地上躺着的狗有多少只？”）。
    \item 场景文本理解数据集。

​TextVQA\cite{singh2019towards}：包含4.5万条问题与2.8万张图像，要求模型识别图像中文本内容（如车牌号），每问题提供10个人工标注答案。

​ST-VQA\cite{biten2019scene}：包含3.1万条场景文本相关问题与2.3万张图像，确保问题无歧义且图像包含足够文本，采用归一化Levenshtein距离作为评估指标，减少答案拼写误差的影响。
\begin{figure}[h]
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth, keepaspectratio]{figures/STVQA-A.png}
        \begin{center}
            \footnotesize 问题：标志牌上写的是什么？\\
            \footnotesize 答案:STOP\\
        \end{center}
        \caption*{(a)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth, keepaspectratio]{figures/STVQA-B.png}
        \begin{center}
            \footnotesize 问题：这趟列车要去哪里？\\
            \footnotesize 答案：New York\\
        \end{center}
        \caption*{(b)}
    \end{subfigure}
    \caption{ST-VQA数据集的示例}
    \centering
    \label{fig:ST-VQA-example}
\end{figure}
\end{enumerate}

\subsubsection{基于知识的数据集}
当仅凭图像信息无法回答问题时，需借助外部知识源获取所需信息。此类场景下，知识库（Knowledge-Based, KB）数据集通过构建依赖外部知识的问题，
旨在训练模型识别知识缺口并有效检索相关知识的能力。值得注意的是，知识库数据集与外部知识库（External Knowledge Bases, EKB）存在本质差异：
前者为特定任务设计的问答数据集，后者为结构化世界知识集合。
典型EKB包括人工标注构建的大规模知识库（如DBpedia [11]、Freebase [16]、Wikidata [127]），
以及从非结构化/半结构化数据自动提取的知识库（如YAGO [48][80]、OpenIE [12][34][35]、NELL [22]、NEIL [25]、WebChild [118]、ConceptNet [76]）。

FVQA\cite{wang2017fvqa} 是典型的知识库数据集，包含2,190幅图像及5,826个需外部知识解答的问题。该数据集为每个图像-问题-答案三元组附加结构化支持性事实（如三元组(Cat, CapableOf, ClimbingTrees)），模拟人类常识推理过程。通过提供显式知识标注，FVQA指导模型从事实数据库中检索相关支持性事实以推导答案。
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.28]{figures/FTVQA.png}
    \begin{center}
        \footnotesize 问题：地面上的红色物体是用来干什么的？\\
        \footnotesize 答案：灭火\\
        \footnotesize 支持性事实：消防栓可用于灭火。
    \end{center}
    \caption{FVQA数据集的示例}
    \label{fig:fvqa-example}
\end{figure}
KB-VQA\cite{wang2015explicit}包含700幅图像对应的2,402个问题，涵盖23类问题模板。其中，1,256个为纯视觉问题，883个需常识推理，263个需查询外部知识库，系统化考察模型对内部与外部知识的协同利用能力。

OK-VQA\cite{marino2019ok}包含14,031幅图像对应的14,055个问题，其规模超越FVQA与KB-VQA。该数据集严格限制答案分布偏差，所有问题均需外部知识解答，重点评估模型在开放领域知识下的泛化能力。

\subsection{生成ASP程序的发展与现状}
自动化代码生成的优势在文献中已得到广泛认可\cite{ernst2022ai}\cite{peng2023impact}\cite{dakhel2023github}，
几乎所有主流编程语言如今都得到了自动化程序合成工具的支持\cite{chen2021evaluating}。
在该领域中，LLM扮演着核心角色，其性能已在文献中被广泛比较\cite{xu2022systematic}\cite{wang2023codet5+}。
针对命令式编程代码生成的模型微调带来的积极影响也已得到证实\cite{ma2024llamoco}。

在声明式编程领域，一个公认的研究目标是开发能够简化和自动化答案集编程（ASP）程序开发的工具，旨在弥合自然语言规范与ASP源代码之间的鸿沟
\cite{erdem2009transforming}\cite{fang2017approach}\cite{schwitter2018specifying}\cite{caruso2024cnl2asp}。
早期的研究提案主要集中于通过将简化英语描述的逻辑谜题翻译为ASP代码来自动化解谜过程\cite{baral2012solving}，
该方法采用了λ演算和概率组合范畴语法。后续研究重点转向为ASP程序开发受控自然语言（CNLs）\cite{kuhn2014survey}，这类语言作为完整自然语言的子集，
具有受限的语法结构和词汇库。其中，BIOQUERYCNL\cite{erdem2009transforming}定义了CNL的语法结构以及将查询转换为ASP的算法；
Fang等人提出了基于LANA注解的CNL方法，并在SeaLion集成开发环境中实现\cite{fang2017approach}。2018年，Schwitter开发了名为PENGASP的CNL，
用于规范化和语言化答案集程序\cite{schwitter2018specifying}。最近，Dodaro等人推出了CNL2ASP——一个可公开获取的广泛工具，用于将受控自然语言转换为ASP程序\cite{caruso2024cnl2asp}。这些CNL一方面为程序员提供了更接近自然语言的编程界面，显著降低了ASP编码门槛；
另一方面，CNL仍存在自身的语法约束，未能完全免除用户按照形式化语法构造语句的要求。

近年来，大型语言模型（LLMs）与答案集编程（ASP）形式的结合应用已展现出多维度创新路径。Nye等人于2021年提出的双系统模型
（Dual-System Model）基于GPT-3架构，通过神经直觉系统（System 1）与逻辑推理系统（System 2）的协同机制，
实现了自然语言向语义解析器的自动转换及其与推理模块的深度集成\cite{nye2021improving}。Yang等学者在2023年进一步验证了GPT-3等模型作为少样本语义解析器的潜力，
可无需针对特定问答任务进行模型重构，直接将自然语言转化为ASP逻辑形式\cite{yang2023coupling}。
Ishay团队同年则开创性地将提示工程技术与LLMs结合，基于Mitra等人的逻辑谜题数据集构建ASP求解系统\cite{mitra2016addressing}。
此外，Rajasekharan等人提出的STAR框架系统探索了LLMs与ASP协同执行自然语言理解任务的实践路径\cite{rajasekharan2023reliable}。

然而，当前研究在声明式编程语言的代码合成领域仍存在显著空白。
Borroto等研究者\cite{borroto2024automaticcompositionaspprograms}于2024年通过NL2ASP工具首次实现突破，该工具采用两阶段架构：
首先通过神经机器翻译（基于T5-small和Bart-base等Transformer模型）将自然语言转换为受控自然语言（CNL），
继而利用CNL2ASP工具生成ASP代码。尽管在图形问题求解领域取得进展，但该工具存在三大局限——领域特定性（仅限于图形问题）、
中间格式依赖（需CNL转换）以及系统化评估缺失。

\subsection{空间推理的发展与现状}
空间推理涉及对物体位置、方向、距离以及相互关系的理解和处理，是计算机视觉、自然语言处理以及机器人导航等领域的核心能力。
尽管人类在空间推理上有天然优势，但当前的模型（尤其是大语言模型和视觉语言模型）在这方面仍面临不少挑战。
这一问题催生了大量针对空间推理能力提升的研究工作。

基于规则的符号推理最早被用于解决空间推理问题，其核心思想是使用几何约束（如欧氏距离、拓扑关系RCC8）和逻辑推理（如谓词逻辑）建模空间关系。
Allen等人提出了相关方法。然而，这种方法过于依赖人工规则，难以处理复杂场景和非结构化数据，实际应用价值有限。后来，有一部分研究者将概率图模型引入到空间推理问题中。
Kuipers等人提出空间语义层次模型，结合贝叶斯网络或马尔可夫随机场（MRF）建模空间关系的不确定性。

进入2010年代，深度学习方法开始快速发展，一些研究者将深度学习方法引入空间推理领域。Antol等人\cite{Antol2015VQA}提出使用CNN+LSTM直接预测
空间问题的答案，Xu等人\cite{xu2017scene}提出使用场景图生成的方法检测物体及其空间关系。这些端到端神经网络的方法，都是黑箱模型，缺乏可解释性，且易受数据偏差影响。
后来有研究者尝试使用图神经网络解决空间推理问题，将场景建模为图结构，通过消息传递捕捉空间关系。

2023年，以OpenAI的ChatGPT广受世界关注为标志，LLM得以迅速发展。以LLM为重要组成部分的VLM得以快速发展，应用于空间推理问题的解决中。
研究者致力于增强VLM的空间推理能力，取得了一系列重要成果。
例如，Google DeepMind开发的SpatialVLM\cite{chen2024spatialvlmendowingvisionlanguagemodels}项目通过生成并训练一个大规模3D空间视觉问答（VQA）数据集，
显著提升了VLM在空间推理任务中的表现。该研究利用互联网规模的真实世界图像，自动标注了约1000万张图像和20亿个VQA示例
（其中50\%为定性，50\%为定量），在二元谓词预测任务中达到了75.2\%的准确率，远超GPT-4V（68.0\%）和LLaVA-1.5（71.3\%）。
此外，SpatialVLM在定量问题中输出数字的比率达到99.0\%，其中37.2\%的答案落在[50, 200]\%的范围内，展示了其在链式思维空间推理和机器人应用中的潜力，
如为机器人任务提供密集奖励注释。

另一项研究SpatialRGPT\cite{cheng2024spatialrgptgroundedspatialreasoning}通过引入数据整理管道和灵活的“插件”模块，
整合深度信息到视觉编码器中，增强了VLM在3D空间认知中的表现。该研究提出了SpatialRGBT-Bench基准，包含室内、外和模拟环境的3D注释，
用于评估VLM在复杂环境下的空间推理能力。这些进展表明，通过大规模数据训练和多模态信息整合，VLM的空间推理能力得到了显著提升。

尽管VLM在空间推理任务中取得了一定的进展，但VLM仍在部分可见场景中存在一些局限性，主要体现在以下几个方面：
\begin{enumerate}[itemsep=0pt,parsep=0pt]
\item 幻觉现象。当视觉信息不完整时，VLM 往往会生成不正确或虚构的答案，
即“幻觉”出图像中不存在或无法从可用视觉线索中可靠推断出的细节。这种趋势在问题涉及完全缺失的对象时可能尤为明显，
尽管模型对答案充满信心，实际上却给出了错误答案\cite{vardi2025clipupclipbasedunanswerableproblem}。
\item 空间关系推理能力下降。理解对象之间如“后面”或“旁边”的空间关系通常依赖于完整或足够的视觉线索。
当场景中的某些部分由于遮挡或视野受限而缺失时，VLM对空间关系的准确辨识能力会受到严重影响，甚至出现逃避或胡乱回答的现象\cite{chen2024spatialvlmendowingvisionlanguagemodels}。
\item 部分基本任务出现失败。有研究指出，在部分可见场景下VLM在一些基本的多对象推理任务（如计数和定位）上的表现不佳\cite{campbell2024understandinglimitsvisionlanguage}。
\end{enumerate}

这些局限性表明，当前基于LLM的VLM仍难以有效应对基于空间推理的VQA任务，特别是在出现遮挡、视野受限、噪声等情形的部分可见场景中。
\subsection{神经符号方法在空间推理中的发展与现状}
为克服上述局限，研究者开始探索神经符号方法，将神经网络的模式识别能力与符号推理的逻辑表达能力相结合。
神经符号方法通过显式表示空间关系和逻辑规则，能够更好地处理复杂推理任务，尤其是在多跳推理和泛化能力方面。

最早将神经符号方法用于空间推理的工作是由Andreas等人\cite{andreas2016neural}提出的Neural Module Network（NMN），其核心思想是：NMN通过模块化分解，将复杂推理拆解为可复用子模块的动态组合，
使模型能根据问题语义动态调整计算路径。最终在CLEVR数据集上实现了超过90\%的准确率，显著优于传统的CNN-LSTM模型。这一开创性的工作，为
后续神经符号推理的研究奠定了基础。

Yi等人\cite{yi2019neuralsymbolicvqadisentanglingreasoning}在Andreas等人的工作基础上，提出了Neural-Symbolic VQA（NS-VQA）模型，
通过设计神经符号解耦框架，将视觉感知、语言理解和符号推理解耦，以减少跨模态耦合带来的偏差放大效应，并通过符号程序将问题转化为可执行的推理步骤，避免端到端模型对语言表面模式的依赖。
最终实验结果表明，NS-VQA在CLEVR数据集上达到了94.5\%的准确率，验证了神经符号方法在空间推理中的有效性。此外，NS-VQA通过分离语言理解和视觉推理，模型对语言分布偏移的敏感性降低，性能提升约15\%。
该成果是神经符号融合的样板性工作，首次在VQA中系统整合符号推理与深度学习，为后续神经符号AI研究提供方法论基础，也为对抗语言先验提供了一种解决方案，
推动VQA从“模式匹配”向“真实理解”演进。

后续，Yang等人\cite{yang2020neurasp}提出了NeurASP这类方法在 NS‐VQA 的基础上引入了神经原子（neural atom）和选择规则，
用以更灵活地处理对象检测的不确定性，缓解了对强监督程序注释的依赖，并在一定程度上改善了符号执行模块的透明性。Thomas等人\cite{eiter2022neuro}对NeurASP中进行改进，在ASP编码和不确定性管理上采用了基于统计门槛的筛选策略，
致力于提高系统的效率和鲁棒性，同时更明显地解耦视觉感知与符号推理，使得推理过程更加透明和可解释。

随着LLM在2023年的迅速发展，越来越多的研究者将LLM引入到神经符号方法中，取代传统的CNN、RNN、LSTM等神经网络。Bauer\cite{bauer2023neuro}等人
利用大型语言模型（LLMs）进行语言解析，相比传统的 LSTM 或其他序列模型，LLMs 能够更准确、灵活地将自然语言问题转化为符号化的查询或图结构表示。这不仅提高了程序生成的质量，还增强了对复杂语言结构和多样化问句的处理能力，实现了神经和符号推理模块的更深层次融合。

尽管将LLM引入神经符号方法后，一系列实验证明LLM在空间推理方面表现不俗，但仍存在一系列问题。
Wang等人\cite{wang2024dspy}在研究中指出，LLM通常只能捕捉语义信息，对于需要精确定义物体位置、方向、距离等空间关系的问题，经常出现理解不足或错误推理。
此外，在需要连续、层次化空间推理的任务中，LLM容易失去推理链条，导致答案不连贯或不准确。另外，
直接通过自然语言生成逻辑代码（例如ASP规则）时，LLM容易生成语法或逻辑不一致的输出，从而使整个推理流程失效。
针对这些问题，他们提出了一个基于DSPy的神经符号管道。
该管道使用LLM将自然语言描述转化为符号化表示（事实和规则），而将具体的逻辑推理任务交给回答集编程（ASP）模块处理，并且利用DSPy框架，通过多轮反馈和自我修正
，改进LLM生成的ASP代码，降低解析错误、变量绑定错误等问题，提高ASP代码的可执行性。
另外，其将LLM在自然语言理解上的优势与ASP在逻辑推理上的精确性相结合，从而在如StepGame和SparQA等空间推理任务上大幅提升准确率。

Wang等人\cite{wang2024dspy}的工作展示了神经符号方法在空间推理中的潜力，大大简化了ASP与LLM进行系统集成的难度，充分发挥了LLM和ASP各自的优势，
为这神经符号方法在空间推理领域的普及做出了积极贡献。然而，其提出的神经符号管道仍然存在一些局限性，
主要体现在不能实现对ASP规则的自动拓展。在每次推理完成后，ASP求解器产生的答案集，可能会包含一些新的事实或规则。而现有的神经符号管道并不能自动将这些新产生的事实或规则反馈存放到本地ASP知识库中，
进而无法利用过去的推理经验来提升后续推理的效率和准确性。
对神经符号管道的进一步改造，可以围绕以上的问题进行进一步深入研究。

总的来看，神经符号方法在空间推理领域展现了强大的潜力，特别是在语言驱动、视觉驱动和机器人应用中。尽管当前存
在挑战，但其在组合性推理和可解释性方面的优势为AI系统的进一步发展提供了重要方向。随着研究的深入，神经符号方法有望在更广泛的现实世界应用中发挥作用。

\section{研究目标与内容}
本课题的主要研究目标是，对现有的VQA数据集进行改进，以考察模型在部分可见场景下回答空间推理问题的能力。
此外，设计一个新的神经符号VQA框架，在已有的框架的基础上，新增对规则进行自动补充的能力，提升后续推理的效率和准确率。最后，
基于所设计的神经符号框架，实现一个VQA课堂教学演示原型系统，为教师在课堂上向学生直观展示人工智能的应用及其推理过程提供支持。

本文的主要研究内容包括以下三个方面：

\begin{enumerate}[label=(\arabic*),itemsep=0pt,parsep=0pt]
    \item 对CLEVR数据集进行改进，引入部分可见性、遮挡机制与复杂空间关系模板，构建覆盖部分场景可见问题的 VQA 数据集，
考察模型能否在充分理解图像内容的基础上，结合已有知识正确回答问题，更好地评估模型对图像和问题的理解能力。
    \item 对现有神经符号框架进行改进，采用知识蒸馏的思想，在ASP求解器进行推理前调用大语言模型，
通过利用ASP知识库中的先验知识对语义解析与视觉场景理解生成的ASP规则进行补充，从而实现对ASP规则的自动拓展能力。
    \item 基于所设计的神经符号框架，实现一个VQA课堂教学演示原型系统，为教师在课堂上进行人工智能的教学提供支持。
\end{enumerate}

\section{研究方法与技术路线}
本文针对以上研究目标和研究内容，综合多种方法进行研究。本文的研究主要涉及三个重点内容：
\begin{enumerate}[label=(\arabic*),itemsep=0pt,parsep=0pt]
    \item 对于视觉问答数据集的构建，首先，用案例分析法研究CLEVR数据集，分析其特点和不足，为新数据集的构建提供理论依据和设计方向。
然后采用文献研究法，调研现有的视觉问答数据集，学习它们在数据集构造过程中的方法和策略。
    \item 对于面向空间推理领域的神经VQA符号框架的研究，首先，分析现有神经符号方法在空间推理方面的优势和不足。
其次，进行模型构建。最后，进行对比实验，从回答问题的准确率角度评估设计的神经符号框架的性能。
    \item 对于视觉问答系统的设计与实现，首先，进行需求分析，明确系统的功能和性能要求。其次，设计视觉问答系统的整体架构与模块划分，明确各模块所需的技术方案。
再次，基于前文的研究成果，实现视觉问答系统的各个模块，并进行系统集成和测试。最后，通过实验验证，评估视觉问答系统的性能和可用性。
\end{enumerate}

具体的技术路线如图\ref{roadmap}所示。

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{process.png}
    \caption{技术路线图\label{roadmap}}
\end{figure}

\section{论文结构}
本文共分为六个章节，各章节的主要内容具体如下：

第一章为绪论，总体介绍本文的研究背景及意义、相关研究现状、研究目标
与研究内容、研究方法与技术路线及本文的结构安排。

第二章为背景知识，对本文涉及到的主要技术进行介绍，具体包括ASP程序语法及ASP求解器、
GLIP以及DSPy。

第三章为数据集构建。对本文所用的视觉问答数据集进行详细介绍，
包括数据集的构造目的、数据集的研究方向、数据集的设计流程以及对数据集质量的验证。

第四章为神经符号VQA框架的研究设计。本章详细介绍本文设计的神经符号VQA框架，
包括总体架构、视觉场景理解、语义解析、知识蒸馏、迭代反馈、规则修正和 ASP 推理等模块，并通过对比试验证明了相比于VLM直接问答，神经符号方法能够有效提升模型在部分可见场景
下回答空间推理问题的正确率，并证明了本文新增的知识蒸馏模块能够进一步提升神经符号方法解决空间推理任务时的性能。

第五章为问答系统的设计与实现。对本文设计的VQA课堂教学演示原型系统进行详细介绍
，包括系统需求分析、系统架构设计、模块划分、模块实现和系统集成等内容。

第六章中对本文工作加以总结，分析本文的创新点和不足之处，并对未来的研究方向进行展望。
