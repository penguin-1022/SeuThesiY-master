%===============================================================================
\categorynumber{000} % 分类采用《中国图书资料分类法》
\UDC{000}            %《国际十进分类法UDC》的类号
\secretlevel{公开}    %学位论文密级分为"公开"、"内部"、"秘密"和"机密"四种
\studentid{222171}   %学号要完整，前面的零不能省略。
\title{面向空间推理领域的}{面向空间推理领域的}{神经符号VQA框架的设计与实现}{神经符号VQA框架的设计与实现}{Research and implementation of visual question answering technology }{based on answer set programming}
\author{贾梁}{Jia Liang}
\advisor{张志政}{}{Zhang Zhizheng}{}
% \coadvisor{张志政}{副教授}{Zhang Zhizheng}{Associate Prof.} % 没有% 可以不填
\degreetype{工程硕士}{Master of Engineering} % 详细学位名称
\thesisform{应用研究} % 包括应用研究、调研报告、规划、产品开发、案例分析、项目管理、文学艺术作品、其它。非专业型硕士可忽略
\major{电子信息}
\submajor{计算机技术}
\defenddate{2025年5月30日}
\authorizedate{2025年6月20日}
\committeechair{翟玉庆}
\reviewer{倪庆剑}{张祥}
\department{东南大学计算机科学与工程学院}{School of Computer Science and Engineering}
\makebigcover
\makecover
\begin{abstract}{多模态，回答集编程，视觉问答，空间推理，神经符号方法}
积木世界是人工智能研究、教学、实验、评估的重要场景，能够模拟实际应用中物体间的空间关系。
积木世界VQA要求视觉语言模型（Visual Language Model, VLM）具备空间推理能力，但已有研究表明，当面对部分可见场景时，
其回答准确率显著下降。目前，通过结合神经网络和符号推理形成的神经符号模型，利用深度学习将视觉信息转化为逻辑符号，
再利用符号推理进行问题求解问题，比单纯依赖深度学习的视觉语言模型在空间推理任务中有更优异的表现。
由于回答集程序（Answer Set Program, ASP）是一种具备非单调推理和高效推理机的符号推理方法，
神经符号模型中采用ASP构建的神经符号VQA框架被寄予厚望。然而，现有框架设计中ASP规则的扩展仍依赖人工，
难以有效应对部分可见场景中基于空间推理的VQA任务。
    
针对上述问题，本文从构建积木世界部分可见场景中基于空间推理的VQA数据集、
设计规则自动补充的神经符号VQA方法、设计实现VQA课堂演示原型系统三个方面开展工作，具体如下：

\begin{enumerate}[itemsep=0pt]
\item 积木世界部分可见场景空间推理VQA数据集（Partial Observation VQA Dataset, POVQAD）构建。CLEVR是空间推理的经典数据集，然而CLEVR中问题涉及的物体属性和物体间空间关系在图像中均完全可见，通过引入部分可见性、遮挡机制与复杂空间关系模板，构建了覆盖部分场景可见问题的VQA数据集POVQAD。该数据集将问题推理所需的平均步骤数从3提升到5，并新增了超 15\% 的空间封闭性与假设类问题，且超过40\%的图像中包含不可见目标，比原数据集更能有效考察模型在部分可见场景中回答空间推理问题的能力。
\item 规则自动补充的神经符号VQA框架（Rule Complement Neuro-Symbolic Pipeline, RCNSP）设计。
RCNSP借鉴现有神经符号VQA框架的架构，新增规则蒸馏模块，
实现在ASP求解器进行推理前调用微调后的大语言模型自动对规则进行补充。
实验表明，在DeepSeek、Llama3、ChatGPT-4o三种大语言模型上，使用RCNSP比直接向VLM提问的准确率平均提升16.5\%，
比现有的神经符号VQA框架的准确率平均提升8.9\%，表明通过规则蒸馏能有效提升神经符号方法在部分可见场景中基于空间推理VQA中的效能。
\item 设计实现了一个VQA课堂演示原型系统。在RCNSP框架和POVQAD数据集基础上，设计实现了VQA原型系统，
该系统支持在针对用户的VQA任务进行空间推理时展示推理的中间步骤和逻辑链条，能够为开展相关研究实验和自动规划课堂教学提供辅助。
初步测试表明该系统在CPU为Intel Core i9-11900K，内存128G，显卡为3张RTX 3090并联的硬件环境下，并发量为50，
90\%响应时间为4.9秒，能够满足课堂教学场景下的用户需要。
\end{enumerate}

\end{abstract}

\begin{englishabstract}{Multi-modal, Answer Set Programming, Visual Question Answering, Spatial Reasoning, Neuro-symbolic Method}
The block world serves as an important scenario for artificial intelligence research, education, experimentation, and evaluation, as it can simulate spatial relationships between objects in real-world applications. Visual Question Answering (VQA) in the block world requires Visual Language Models (VLMs) to possess spatial reasoning capabilities. However, existing studies have shown that the answer accuracy of VLMs significantly degrades when facing partially observable scenes. Currently, neuro-symbolic models that combine neural networks with symbolic reasoning—using deep learning to transform visual information into logical symbols and then employing symbolic reasoning for question answering—have demonstrated superior performance in spatial reasoning tasks compared to purely deep learning-based VLMs. As Answer Set Programming (ASP) is a symbolic reasoning approach with non-monotonic reasoning capabilities and efficient solvers, ASP-based neuro-symbolic VQA frameworks are regarded as highly promising. Nevertheless, current frameworks still rely on manually designed ASP rules, which limits their effectiveness in handling spatial reasoning VQA tasks under partial observability.

To address the above challenges, this thesis conducts work in three main aspects: constructing a spatial reasoning VQA dataset in a partially observable block world, designing a neuro-symbolic VQA approach with automatic rule completion, and implementing a prototype classroom demonstration system for VQA. The specific contributions are as follows:
\begin{enumerate}[itemsep=0pt, parsep=0pt]
\item Construction of Partial Observation VQA Dataset (POVQAD).
CLEVR is a classical dataset for spatial reasoning. 
However, in CLEVR, all object attributes and spatial relations are fully visible in the images. 
By introducing partial visibility, occlusion mechanisms, and complex spatial relation templates, this work constructs POVQAD—a VQA dataset tailored for partially observable scenes. 
POVQAD increases the average number of reasoning steps per question from 3 to 5, adds over 15\% more questions involving spatial closure and hypothetical reasoning, and includes over 40\% images with invisible targets, thereby providing a more effective benchmark for evaluating models' spatial reasoning capabilities under partial observability.
\item Design of Rule Complement Neuro-Symbolic Pipeline (RCNSP).
RCNSP builds upon existing neuro-symbolic VQA frameworks by incorporating a rule distillation module, which leverages fine-tuned 
large language models (LLMs) to automatically supplement ASP rules before reasoning. 
Experiments with DeepSeek, LLaMA3, and ChatGPT-4o demonstrate that RCNSP improves answer accuracy by an average of 16.5\% compared to directly querying VLMs, 
and by 8.9\% over existing neuro-symbolic frameworks, 
showing that rule distillation significantly enhances performance in spatial reasoning VQA under partial observability.
\item Implementation of a VQA Classroom Demonstration System. Based on the RCNSP framework and POVQAD dataset, a prototype VQA system is implemented to support classroom demonstrations. The system visualizes intermediate reasoning steps and logical chains when handling spatial reasoning questions, providing assistance for research experiments and instructional planning. Preliminary tests show that on a hardware setup with an Intel Core i9-11900K CPU, 128GB RAM, and three RTX 3090 GPUs, the system achieves a 90th percentile response time of 4.9 seconds under a concurrency of 50, meeting the performance needs of classroom teaching scenarios.
\end{enumerate}
\end{englishabstract}

\setnomname{术语与符号约定}
\tableofcontents
\listofothers
%===============================================================================
